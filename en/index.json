[{"content":"","date":null,"permalink":"/en/tags/android/","section":"Tags","summary":"","title":"Android"},{"content":" We know that the screen displays content by emitting RGB lamp beads one by one, and the brightness of the lamp beads is determined by a memory area. By writing data to this memory area, we can observe the data display effect on the screen. This is a complex and flexible job. In order to facilitate this work, the pioneers developed the OpenGL standard, and our story will start from here.\nOpenGL ES #OpenGL ES is a streamlined version of OpenGL. The Android platform has provided support for OpenGL ES since its release. However, different versions support different OpenGL ES versions. The current mainstream versions are still 2.0 and 3.0. OpenGL ES is a set of APIs that provide developers with the ability to configure data, transfer data, and draw content. Its work is strictly related to drawing, so OpenGL ES alone will not cause a big obstacle to understanding. The problem lies in the configuration environment for configuring OpenGL ES. Why should we separate the OpenGL ES API and the configuration environment? Because OpenGL ES is a cross-platform API, but it needs to be bound to a specific platform, such as Android, for actual operation. The conditions required to prepare the OpenGL ES environment are different between platforms. In order to ensure the cross-platform capabilities of OpenGL ES, it is necessary to separate the configuration environment and bind it to a specific platform. On Android, this configuration environment is EGL. Clarifying the difference between the OpenGL ES API and configuring the OpenGL ES environment is not only very helpful in understanding these two key concepts, but also greatly helps in debugging the code and troubleshooting later.\nWorkflow #After clarifying some basic concepts, our next most important task is to clarify the workflow of OpenGL ES. Many tutorials start by listing a lot of nouns or giving examples directly, which I think is inappropriate. Only by becoming familiar with the workflow can we know what to do when writing code and locate problems faster and more accurately during debugging.\nPrepare environment #OpenGL ES is composed of a series of APIs, but it does not mean that these APIs can be called at any time. Instead, it requires some settings in the running environment, which is the preparation environment. Preparing the environment usually involves doing some video memory allocation and window configuration work, which is very tedious but essential.\nPrepare shader #Shaders are very important, but for beginners, you don’t need to pay too much attention to them. Many effects can be directly found in ready-made codes online, but how to assemble these codes into a complete runnable program is not necessarily known. . We just need to be clear that shaders are an important part of OpenGL ES development, and this is where the magic happens.\nPrepare program #Although shaders are important, they cannot run independently and need to be managed by a program. The program in question is an OpenGL ES object that is responsible for putting the shaders together. This object is required before running most OpenGL ES APIs.\nrendering #The rendering process is actually preparing data. We need to assign some data used in the shader, and then call the drawing API to complete the final drawing work. The GPU will pass the data to the shader, and the shader will go through the pipeline to convert the data into final display data and store it in the video memory.\nDisplay #Rendering does not mean that the data is displayed, but that the data is calculated. To see the calculated data on the screen, you may need to call a function in the OpenGL ES environment configuration tool, such as exchanging buffers or switching display objects.\nClean up #Like memory, we will also apply for some resources when using the OpenGL ES API. After rendering, we should actively release the resources for subsequent program use. Many times when our normal application for resources fails, it may be because the previous resources have not been released.\nThe above is the general process for developing OpenGL ES applications. Since OpenGL ES development is difficult to debug, the most effective way to locate problems when discovering them is to determine the error link and then deal with it in a targeted manner. So it’s important to be familiar with the process.\nGet started with examples #Since there are many concepts related to OpenGLES, in order to minimize the interference of related concepts, this article intends to focus on only the first step in the above process. At the same time, use the knowledge points involved to implement a minimal example-dye the window red.\nLet’s start with the first concept—EGL.\nEGL #OpenGL ES is only an abstraction of drawing and does not provide an abstraction of the running environment. If you want to apply for video memory, you need to specify where the video is stored. After the image is calculated, you also need to specify where it will be displayed. EGL is a collection of these environmental abstractions. In order to explain the relevant concepts in a popular way, we can play a role-playing role-what should we do if we are asked to design relevant standards.\nThe first thing that is easy to think of is that we need a monitor, because OpenGL ES will eventually generate a set of color data. If we want to see these colors, we definitely need a monitor to display these color data. At the same time, we know that monitors also have many specifications and features. In order to be compatible with various low-end to high-end monitors, it is definitely necessary to make a layer of abstraction for it and provide some methods for setting properties. This is EGLDisplay\u0026rsquo;s task.\nAfter determining the display, we will find that we can only select the entire display or not use it every time. When we actually use it, there must be situations where only one area is displayed, or multiple areas are displayed at the same time. In order to meet this usage scenario , it is necessary to further divide the display so that it can support the simultaneous operation of multiple areas, and the person responsible for this layer of abstraction task is EGLSurface .\nSince they all support multiple areas, it must be possible to configure the areas. Maybe on the same monitor, one area only needs to display black and white pixels, and another area needs to display high-definition pictures. In order to make these configurations effective and independent of each other, An abstraction is definitely needed, which must be able to save the display configuration and isolate the OpenGL ES environment so that API calls to OpenGL ES in one area will not affect another area. This is EGLContext .\nThe above are the three core concepts of EGL, which are the abstraction of the display, display area, and display configuration.\nThe above concepts are all scattered, and we definitely need to connect all the parts together in actual work, so it is necessary to make a summary of their work processes. First we need to obtain a EGLDisplay to determine the final display device, and then configure a display area EGLSurface according to the configuration supported by the display device. Finally, use EGLContext to connect EGLDisplay and EGLSurface . Once the association is successful, it means that the OpenGL ES environment is ready. The next step is to create a shader, create a shader program, and prepare for drawing.\nAfter sorting out the process, let\u0026rsquo;s take a look at how to write the code. In order to minimize the barriers to understanding, this article will use the Java-side interface as an example.\nPrepare EGLDisplay #Learning any new skill requires an entrance, and the common entrances for OpenGLES and EGL are EGLDisplay . So the first step is to get a EGLDisplay object. We cannot create this object directly, but need to obtain an object through the eglGetDisplay method. This object is very important and is the first parameter of almost all subsequent EGL-related APIs. Therefore, it usually needs to be cached for subsequent use. Although the EGLDisplay object already exists, it cannot be used directly. Need to call eglInitialize for initialization. This phenomenon is also very common in many SDKs. After obtaining the object, it needs to be initialized to ensure that the internal state is restored to the initial state.\nGet configuration #After successfully calling the eglInitialize method, the EGLDisplay object is ready and the display area can be configured. But we don’t know which configuration information is valid and which configuration information is supported, because different hardware supports different features. If we directly hard-code the configuration regardless of the hardware features, the code may fail to run on a certain device. This is not What we want to see. Therefore, in order for the configuration to be valid on all devices, the effective way is not for us to specify the configuration, but for us to actively query the hardware to see if it is the configuration we want, that is, let the EGLDisplay object tell us.\nEGLDisplay provides two methods to query the configuration supported by the hardware. One is to directly obtain all configuration information supported by the device eglGetConfigs , and the other is for the developer to list the desired configuration. Then actively query whether the device supports the listed configurations eglChooseConfig . Developers can choose any method to determine the configuration items of the display area. If the method call is successful, it is equivalent to determining the configuration items of the display area, and we can use these configuration items to configure EGLSurface .\nConfigure display area #Surface is used on the Android platform to represent the display area, but usually we do not deal directly with Surface , but use SurfaceView . However, there are limitations to using SurfaceView . Surface only valid after the surfaceCreated and before surfaceDestroyed of the SurfaceHolder in the SurafceView.That is to say, the operation of configuring the display area can only be performed after receiving the surfaceCreated callback.\nTo configure the display area, you need to use the eglCreateWindowSurface method. The first two parameters are the objects we obtained in the above two steps. The third parameter is a Surface related parameter, which can be Surface , SurfaceHolder . In addition, you can also use the fourth parameter to pass some configuration information about Surface . After the function call is successful, we obtain a EGLSurface object.\nConnect them together #So far, the EGLDisplay object and the EGLSurface object are still independent. The latter only obtains some configuration information from the former, and has no other connection. In order to associate the two together, we need to borrow the EGLContext object. Similarly, creating a EGLContext object requires passing the eglCreateContext function. The first two parameters are the EGLDisplay and EGLConfig obtained in the previous step. , the special one is the third parameter. The third parameter is EGLContext , usually EGL_NO_CONTEXT is passed, which means creating an independent EGLContext object. Another situation is that when two rendering environments want to share resources, EGL_NO_CONTEXT should be passed normally when creating the first rendering environment. When creating the second rendering environment, you need to pass the EGLContext object is passed in, then the second rendering environment can use the textures, shaders, shader programs, and buffer class objects created in the first rendering environment, which are shared by the two rendering environments. got some data. At this point, three important objects have appeared, but they are not connected to each other yet, so the eglMakeCurrent function is needed to complete this work. This function will bind the EGLContext object to the current thread and also bind the EGLContext object to EGLSurface . After the binding is completed, the three major The objects are connected and the OpenGL ES environment is ready.\nBefore entering the world of OpenGL ES, we finally review the previous EGL world in code.\nval display = EGL14.eglGetDisplay(EGL14.EGL_DEFAULT_DISPLAY) if (EGL14.EGL_NO_DISPLAY == display) { log() return } val versions = IntArray(2) var flag = EGL14.eglInitialize(display, versions, 0, versions, 1) if (!flag) { log() return } Log.i(TAG, \u0026#34;EGL version:major = ${versions[0]}, minor = ${versions[1]}\u0026#34;) val attr = intArrayOf( EGL14.EGL_RED_SIZE, 8, EGL14.EGL_GREEN_SIZE, 8, EGL14.EGL_BLUE_SIZE, 8, EGL14.EGL_NONE ) val configs=Array\u0026lt;EGLConfig?\u0026gt;(1,{null}) val numConfig=IntArray(1) flag = EGL14.eglChooseConfig(display, attr, 0, configs, 0, 1, numConfig, 0) if (!flag) { log() return } val config=configs.first() val eglSurface=EGL14.eglCreateWindowSurface(display,config,surface, intArrayOf(EGL14.EGL_NONE),0) if (EGL14.EGL_NO_SURFACE == eglSurface) { log() return } val context=EGL14.eglCreateContext(display,config,EGL14.EGL_NO_CONTEXT, intArrayOf(EGL14.EGL_NONE),0) if (EGL14.EGL_NO_CONTEXT == context) { log() return } flag = EGL14.eglMakeCurrent(display, eglSurface, eglSurface, context) if (!flag) { log() return } Enter the world of OpenGL ES #After a long preparation, we finally prepared the rendering environment and can use the OpenGL ES API normally. Typically, this is followed by creating shaders and shader programs. Of course, different rendering scenarios usually call different APIs. In this article, we want to dye the window red, so there is no need to create these things. We only need to call two APIs. glClearColor Set the clear screen color.glClear Set the clear screen bit.\nOf course, these two functions alone are not enough. We haven\u0026rsquo;t set the drawing area yet. Yes, you can specify the drawing area separately for each drawing. For example, for the first drawing, we draw in the upper left corner, and for the second drawing, we can draw in the lower right corner. You only need to specify the drawing area before drawing. The drawing area The specification will be valid until the next re-specification, and the function used is glViewport . The first two parameters of the function specify the starting position, and the last two parameters are the distance from the starting position.\nWith the help of these three functions, OpenGL ES will dye our dark black frame red. Let\u0026rsquo;s take a look at the code\n//We want to render the entire area, so the start point is the top left corner and the end point is the width and height of the view GLES20.glViewport(0,0,width,height) //Colour range is 0-1 GLES20.glClearColor(1f,0f,0f,1f) GLES20.glClear(GLES20.GL_COLOR_BUFFER_BIT) Display #We have dyed the window red in the previous section, but after running the application, we will find that the display is still black. That is because we forgot the final screen operation. Because after entering the OpenGL ES world, the logical thing to do is to call the OpenGL ES API. However, the fact is to always remember that the OpenGL ES API is only responsible for drawing. For display-related issues, you have to contact EGL. After OpenGL ES drawing is completed, you need to use eglSwapBuffers to complete the screen operation.\nSummary #This article is the first in the OpenGL ES series. It focuses on my general understanding of EGL and OpenGL ES. The expression may not be so rigorous. It is intended to help readers build a channel to enter this field and understand some main concepts. Basic impression, we will go into each link one by one in the later stage, hoping to have the effect of attracting new ideas.\nAfter reading this article, readers should have a simple impression of the process of developing OpenGL ES applications: EGL environment preparation, shaders, shader programs, rendering, screen loading, and cleanup. Of course, this article only focuses on the preparation of the EGL environment.\nRegarding EGL environment preparation, we have three objects, which are the display, display area, and display context, corresponding to EGLDisplay , EGLSurface , and EGLContext . Environment preparation mainly starts from EGLDisplay , obtains and configures these three objects, and finally uses eglMakeCurrent to associate them. Of course, after using the OpenGL ES API to complete rendering, remember to use `eglSwapBuffers`` to complete the screen operation.\nPlease refer to here for the source code.\n","date":"9 May 2023","permalink":"/en/post/learning%20opengles%20on%20the%20android-fill%20the%20window.html","section":"Posts","summary":"We know that the screen displays content by emitting RGB lamp beads one by one, and the brightness of the lamp beads is determined by a memory area.","title":"Android OpenGLES learning-draw a color"},{"content":"","date":null,"permalink":"/en/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":null,"permalink":"/en/","section":"Deep thinking","summary":"","title":"Deep thinking"},{"content":"","date":null,"permalink":"/en/tags/opengles/","section":"Tags","summary":"","title":"OpenGLES"},{"content":"","date":null,"permalink":"/en/categories/opengles/","section":"Categories","summary":"","title":"OpenGLES"},{"content":"","date":null,"permalink":"/en/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":null,"permalink":"/en/tags/study/","section":"Tags","summary":"","title":"study"},{"content":"","date":null,"permalink":"/en/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/en/tags/c++/","section":"Tags","summary":"","title":"C++"},{"content":"","date":null,"permalink":"/en/categories/c++/","section":"Categories","summary":"","title":"C++"},{"content":"","date":null,"permalink":"/en/tags/%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97/","section":"Tags","summary":"","title":"学习指南"},{"content":" 模板作为C++重要的特性，一直有着举足轻重的地位，是编写高度抽象代码的利器。\n什么是模板 #模板在现实生活中就是范例：把都一样的部分固定起来，把变动的部分空出来，使用时将两部分合起来组成有效的东西。如申请书，Word模板都是这种形式。C++中的模板也是如此，不过更明确的是C++中的模板，变动的部分是一个代指类型的东西，称之为泛型参数。\n我们先从一个例子来看一看模板是怎样发展而来的。如我们需要计算两个对象相加的结果，该如何写代码呢？在写代码前，我们有几个问题需要讨论清楚： 首先我们需要确定的是这两个对象是什么类型，毕竟C++是强类型的编程语言，变量，函数，类都是要明确指定类型是什么的，不确定的类型编译就不能通过。我们先假设这两个类型是整型。确定了类型之后，还需要确定这两个对象需要怎样加起来，根据我们假设的整型，我们知道可以直接调用运算符+。最后我们需要确定，两个对象相加后的结果类型是什么，整型相加的结果也是整型。综上，这个例子的代码看起来可能是这样的\nint sum(int left,int right){ return left + right; } 这个例子很简单，简单到甚至都不需要单独写成一个函数。如果我们需要计算的数据不是两个数，而是一个数组的和呢？基于前面的分析和假设，我们也能很快实现相应的代码\nint sum(const int data[], const std::size_t length) { int result{}; for (int i = 0; i \u0026lt; length;++i) { result += *(data + i); } return result; } 同样很简单。但是遗憾的是，这个函数通用性不强，它只能计算整型的数组和，假如我们需要计算带有小数点的数组和，它就不灵了，因为第一个参数类型不匹配，尽管我们知道sum的代码几乎都能复用，除了第一行的int需要替换成double。但是不能！我们只能复制一份，然后把int的地方改成double。\ndouble sum(const double data[], const std::size_t length) { double result{}; for (int i = 0; i \u0026lt; length;++i) { result += *(data + i); } return result; } 这时你就会发现问题了，这个过程，我们仅仅改变了类型信息。这样的问题还会继续增加，我们可能又需要求float的数组和，上面那个double的数组和同样匹配不了，因为float，double是两个类型。正是因为数据类型不一样，所以很多时候我们需要为不同的数据提供相似的代码，这在数据类型膨胀的情况下是很痛苦的，当对算法进行修改的时候我们需要保证所有的数据类型都被修改到，并且要逐个进行测试，这无疑会增加工作量，并放大错误率。但是实际有效的代码都是要明确类型的，如果类型不明确，编译器就没法确定代码是否合法，不确定的事情编译器就要报错，所以按照普通的思路，这个问题是无解的。 但是其实很多时候，这些相似的代码仅仅是数据类型不一样而已，对付这种重复的工作应该让给计算机来完成，也就是编译器。所以我们需要一种技术，让编译器先不管具体类型是什么，而是用一种特殊的类型来替换，这个类型可以替换成任何类型，用这个特殊的类型完成具体的算法，在使用的时候根据实际的需求，将类型信息提供给算法，让编译器生成满足所提供类型的具体算法，而这就是模板。这和生活中的模板思想上是共通的。算法是固定的部分，数据类型是可变的部分，两个合起来就是合法的C++代码。也就是利用模板，我们可以只写一个算法，借助编译器生成所有类型的算法，这些算法之间唯一不同的就是类型。 当然光有模板还不够，上面只解决了类型的问题，没有解决算法实现的问题。怎么说呢，如我们有一个需求，需要将数据先排序，再查找最大值。这对于数字（int,float,double等）类型是有效的，直接使用比较运算符（\u0026lt;,\u0026gt;）就可以完成了，但是假如想让这个算法适用于自定义类型呢？直接在模板实现中写比较运算符对自定义类型是无效的，因为自定义类型没有实现相对应的比较运算函数。解决方法也很简单，自定义类型实现相对应的比较运算符就行了。诸如此类的问题，在模板中会经常遇到，因为我们对类型的信息一无所知，但是又要确保几乎所有的类型都能正常运作，这就不得不运用各种技术对类型进行限定或者检测，这其实才是模板问题的精髓。所以模板问题不仅仅是类型问题，还是其他C++问题的综合体，需要对C++特性有着较为完整的理解，才能写出有用高效的代码。 C++中通常将模板分为函数模板和模板类，我们先从比较简单的函数模板开始认识。\n函数模板 #函数模板是一种函数，和普通函数不一样的地方是，它的参数列表中至少有一个是不确定类型的。我们用开头的例子来小试牛刀：\ntemplate \u0026lt;typename T\u0026gt; T sum(const T data[], const std::size_t length) { T result{}; for (int i = 0; i \u0026lt; length;++i) { result += *(data + i); } return result; } int main() { int intData[] = { 1, 1, 2, 2 }; float floatData[] = { 1, 1, 2, 2 }; double doubleData[] = { 1, 1, 2, 2 }; auto len = sizeof(intData)/sizeof(intData[0]); std::cout \u0026lt;\u0026lt; \u0026#34;intSum = \u0026#34; \u0026lt;\u0026lt; sum\u0026lt;int\u0026gt;(intData,len) \u0026lt;\u0026lt; \u0026#34;, floatSum = \u0026#34; \u0026lt;\u0026lt; sum\u0026lt;float\u0026gt;(floatData,len) \u0026lt;\u0026lt;\u0026#34;, doubleSum = \u0026#34; \u0026lt;\u0026lt;sum\u0026lt;double\u0026gt;(doubleData,len)\u0026lt;\u0026lt;std::endl; return 0; } // 输出 // intSum = 6, floatSum = 6, doubleSum = 6 在这里，我们仅仅写了一个函数，就可以同时适用于int，float，double。如果还有其它类型实现了默认初始化和运算符+=就同样可以使用这个函数来求和，不需要改动任何现有代码，这就是模板的魅力。 在继续看新东西前，我们先来认识一下函数模板和普通函数之间有什么不同：\n函数模板需要一个模板头，即template\u0026lt;typename T\u0026gt;。它的作用是告诉编译器下面的函数中遇到T的地方都不是具体类型，需要在调用函数时再确定。 函数声明中，类型位置被T替代了，也就是说T是一个占位类型，可以将它当作普通类型来用。在写模板代码时，这是很有用的。 再来看使用函数的地方，也就是类似sum\u0026lt;xxx\u0026gt;(xxxData,len)的语句，其中的xxx代表数据类型，也就是函数模板中T的实际类型。简单来说就告诉编译器，用类型xxx替换函数模板中的类型T，这个过程有个官方的名字，实例化,这是另一个和普通函数不一样的地方.。用函数模板是需要经过两个步骤的。\n定义模板。这一步没有具体类型，需要使用一个泛型参数来对类型占位，也就是只要是出现实际类型的地方，都要使用泛型参数来占位，并用这个泛型参数来实现完整的算法。这一步编译器由于不知道具体类型，不会对一些类型操作进行禁止，而只是检查标识符是否存在，语法是否合法等。 实例化。实例化的过程只会发生在开发者调用函数模板的地方，没有实例化的函数模板的代码是不会出现在最执行文件中的。编译器会对每一处发生实例化的地方，用实际参数来替换泛型参数，并检查实际类型是否支持算法中所有的操作，如果不支持，则编译失败，需要开发者实现相关的操作或者修改函数模板。如上例中，假如我们用一个自定义类型来实例化，就会发现编译无法通过，因为自定义类型没有定义操作符+=（除非该操作符已经被定义了），这个过程就发生在实例化。解决方案也很简单，对自定义类型添加操作符+=即可。 类型推导 #在上例中，我们发现在实例化的过程中，要同时给函数模板传递类型参数和数据参数，并且类型参数往往和数据的类型是一一对应的，这中冗杂的语法对于现代C++来说是不可接受的，所以现代C++编译器都支持类型推导。类型推导可以让开发者省略类型参数，直接根据数据类型来推导出类型参数，所以上例实例化都可以写成sum(xxxData,len)的形式，编译器能分别推导出xxx的类型是int,float，double。 当然类型推导也不是万能的，我们来看下面这个例子\ntemplate \u0026lt;typename T\u0026gt; T max(T a, T b) { return a \u0026gt; b ? a : b; } int main() { int a = 1; int b = 2; std::cout \u0026lt;\u0026lt; \u0026#34;max(\u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; \u0026#34;,\u0026#34;\u0026lt;\u0026lt;b\u0026lt;\u0026lt;\u0026#34;) = \u0026#34; \u0026lt;\u0026lt;max(a,b) \u0026lt;\u0026lt; std::endl; return 0; } // 输出 // max(1,2) = 2 这个例子很直观，结果当然也毫无意外。现在我们要变形了：我们把变量b的类型改为float，就会发现编译无法通过了。提示我们数据类型不匹配，因为a是int，b是float，所以推导出的结果就是max\u0026lt;int,float\u0026gt;()，而实际上我们是只有一个类型参数的。 那既然问题很明了，解决方法也似乎很简单，给max再加一个参数不就行了吗？我们来看一看。\ntemplate \u0026lt;typename A,typename B\u0026gt; A max(A a, B b) { return a \u0026gt; b ? a : b; } int main() { int a = 1; float b = 2; std::cout \u0026lt;\u0026lt; \u0026#34;max(\u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; \u0026#34;,\u0026#34;\u0026lt;\u0026lt;b\u0026lt;\u0026lt;\u0026#34;) = \u0026#34; \u0026lt;\u0026lt;max(a,b) \u0026lt;\u0026lt; std::endl; return 0; } // 输出 // max(1,2) = 2 经过这样改之后，编译和运行都不报错了，问题似乎解决了，是吗？ 并不是，我们把float b = 2;换成float b = 2.5;，\nint main() { int a = 1; float b = 2.5; std::cout \u0026lt;\u0026lt; \u0026#34;max(\u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; \u0026#34;,\u0026#34;\u0026lt;\u0026lt;b\u0026lt;\u0026lt;\u0026#34;) = \u0026#34; \u0026lt;\u0026lt;max(a,b) \u0026lt;\u0026lt; std::endl; return 0; } // 输出 // max(1,2.5) = 2 再次运行程序，就会发现输出是错误的了。因为函数模板中，我们把返回值定义成了A，在实例化的时候A被推导成了int类型，所以实际上max的返回值就成了int类型，最大值B就被从float强制转换成了int类型，丢失了数据精度。那有没有解决方法呢？有的，而且不止一种! 根据上面的分析，其问题的根本是数据被强转了，解决方案当然就是阻止它发生强转，也就是保持两种数据类型是一致的，那怎么保证呢？阻止编译器的类型推导，手动填写类型参数。\nint main() { int a = 1; float b = 2.5; std::cout \u0026lt;\u0026lt; \u0026#34;max(\u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; \u0026#34;,\u0026#34;\u0026lt;\u0026lt;b\u0026lt;\u0026lt;\u0026#34;) = \u0026#34; \u0026lt;\u0026lt;max\u0026lt;float\u0026gt;(a,b) \u0026lt;\u0026lt; std::endl; return 0; } // 输出 // max(1,2.5) = 2.5 可以看到在此例中，我们只填写了一个类型参数，因为类型B会自动推导成float。没错，类型推导是可以部分禁用的！ 另一种解决方案就是完全让编译器计算类型。怎么计算呢，C++11提供了auto和decltype。auto可以计算变量的类型，decltype可以计算表达式的类型，用法如下：\nauto a=1; // a被推导成int类型 auto b=1.5; // b被推导成double类型 decltype(a+b) //结果是double类型 也就是可以将返回值置为auto，然后让编译器决定返回类型\ntemplate \u0026lt;typename A,typename B\u0026gt; auto max(A a, B b) { return a \u0026gt; b ? a : b; } int main() { int a = 1; float b = 2.5; std::cout \u0026lt;\u0026lt; \u0026#34;max(\u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; \u0026#34;,\u0026#34;\u0026lt;\u0026lt;b\u0026lt;\u0026lt;\u0026#34;) = \u0026#34; \u0026lt;\u0026lt;max\u0026lt;float\u0026gt;(a,b) \u0026lt;\u0026lt; std::endl; return 0; } // 输出 // max(1,2.5) = 2.5 假如编译器只支持C++11的话，会麻烦一点，不仅要前置auto，在函数头后还要使用decltype来计算返回类型，这个特性称为尾返回推导。\ntemplate \u0026lt;typename A,typename B\u0026gt; auto max(A a, B b)-\u0026gt;decltype(a + b) { return a \u0026gt; b ? a : b; } 这里decltype里面写的是 函数模板暂时放一放，我们来看一看类模板是怎样的。\n类模板 #和函数模板一样，类模板也至少包含一个泛型参数，这个泛型参数的作用域是整个类，也就是说可以使用这个泛型参数定义成员变量和成员函数。\ntemplate \u0026lt;typename T\u0026gt; class Result { T data; int code; std::string reason; public: Result(T data, int code = 0, std::string reason = \u0026#34;success\u0026#34;) :data{ data }, code{ code }, reason{ reason } { } friend std::ostream\u0026amp; operator\u0026lt;\u0026lt;(std::ostream\u0026amp; os, const Result result) { os \u0026lt;\u0026lt; \u0026#34;Result(data = \u0026#34; \u0026lt;\u0026lt; result.data \u0026lt;\u0026lt;\u0026#34;, code = \u0026#34; \u0026lt;\u0026lt; result.code \u0026lt;\u0026lt;\u0026#34;, reason = \u0026#34; \u0026lt;\u0026lt; result.reason \u0026lt;\u0026lt;\u0026#34;)\u0026#34; \u0026lt;\u0026lt; std::endl; return os; } }; int main() { Result\u0026lt;int\u0026gt; result{ 9527 }; std::cout \u0026lt;\u0026lt; result \u0026lt;\u0026lt; std::endl; return 0; } // 输出 // Result(data = 9527, code = 0, reason = success) 可以看到，类模板和普通类类似，普通类有的它都有——成员函数，成员变量，构造函数等等，值得一说的依然是这个泛型参数T。上例是SDK中常见的数据类，用于指示操作是否成功并且必要时返回操作结果。对于返回一般数据类型，这个类已经足够了，但是假如我们的某个接口无返回值，按照传统即返回void类型，问题出现了。data的实际类型是void，但是我们找不到任何值来初始化它。更进一步，返回void的时候，我们根本不需要data这个成员变量。为了解决类似这种问题，模板提供了特化。\n特化和偏特化 #特化就是用特定类型替代泛型参数重新实现类模板或者函数模板，它依赖于原始模板。如上例中，我们已经有了原始模板类Result\u0026lt;T\u0026gt;，为了解决void不能使用的情况，我们需要为void类型重新定义一个Result，即Result\u0026lt;void\u0026gt;，则Result\u0026lt;void\u0026gt;就称为Result\u0026lt;T\u0026gt;的一种特化，原来的Result\u0026lt;T\u0026gt;称为原始模板类。这样的特化版本可以有很多个，一个类型就是一个特化版本，它完美融合了通用性和特殊性两个优势。当实例化过程中，如果实例化类型和特化类型一致，则实例化将使用特化的那个类（函数）来完成，如下面的例子\n// Result定义保持不变，新增特化版本 template \u0026lt;\u0026gt; class Result\u0026lt;void\u0026gt;{ int code; std::string reason; public: Result(int code = 0, std::string reason = \u0026#34;success\u0026#34;): code{ code }, reason{ reason }{} friend std::ostream\u0026amp; operator\u0026lt;\u0026lt;(std::ostream\u0026amp; os, const Result result) { os \u0026lt;\u0026lt; \u0026#34;Result(\u0026#34;\u0026lt;\u0026lt;\u0026#34;code = \u0026#34; \u0026lt;\u0026lt; result.code \u0026lt;\u0026lt; \u0026#34;, reason = \u0026#34; \u0026lt;\u0026lt; result.reason \u0026lt;\u0026lt; \u0026#34;)\u0026#34; \u0026lt;\u0026lt; std::endl; return os; } }; int main() { Result\u0026lt;void\u0026gt; voidResult; Result\u0026lt;int\u0026gt; intResult{9527}; std::cout \u0026lt;\u0026lt; \u0026#34;void = \u0026#34;\u0026lt;\u0026lt; voidResult\u0026lt;\u0026lt;std::endl\u0026lt;\u0026lt;\u0026#34;int = \u0026#34; \u0026lt;\u0026lt; intResult \u0026lt;\u0026lt; std::endl; return 0; } // 输出 // void = Result(code = 0, reason = success) // int = Result(data = 9527, code = 0, reason = success) 可以看到，当实例化为int类型时，使用的是原始的模板类。而当实例化为void类型时，使用的是特化的版本。 除了特化，还有偏特化。偏特化和特化很像，就是对类型进行一个更窄的限定，使之适用于某一类类型，如const，指针，引用等。或者对有多个泛型参数的类进行部分特化。 特化和偏特化是对模板特殊类型的补充，解决的是模板实现上的一些问题。很多时候如果通用模板不好实现，可以考虑使用特化。当然，特化版本越多，模板的维护成本就越高，这时候就该考虑是否是设计上存在缺陷了。\n类型限定 #C++模板的强大不仅仅表现在对类型的操作上，有时候为了防止我们的类被滥用，我们还需要对这些能力做一些限定，比如禁止某些特定的类型实例化。 在上面的例子中，假设我们规定Result必须返回实际的数据，禁止void实例化该怎么做呢？容易想到的是，我们首先需要一种方法判断实例化时的类型是否是特定类型，然后需要在实例化类型是禁止类型时告诉编译器编译失败。所有的这些，标准库type_traits都提供了支持。它提供了一系列工具来帮助我们识别类型参数，如数字，字符串，指针等等，也提供了一些其他工具辅助这些类型参数工具完成更复杂的功能。 此例中，我们希望实例化类型不能是void，经过查找type_traits，我们发现有个is_void的类，它有个value常量，这个常量在类型参数为void是为true，否则为false。当然有了判定方法还不够，我们还需要在类型不匹配时让编译器报错的方法，恰好，我们有enable_if_t。它有两个类型参数，第一个是布尔表达式，第二个是类型参数。当表达式为真时，类型参数才有定义，否则编译失败。所以为了完成禁止void实例化的功能，我们需要借助两个工具，is_void判断类型参数是否是void,enable_if_t完成布尔表达式到类型参数的转换。综上，让我们来看看实现：\ntemplate \u0026lt;typename T\u0026gt; class Result { std::enable_if_t\u0026lt;!std::is_void\u0026lt;T\u0026gt;::value,T\u0026gt; data; int code; std::string reason; public: Result(std::enable_if_t\u0026lt; !std::is_void\u0026lt;T\u0026gt;::value,T\u0026gt; data, int code = 0, std::string reason = \u0026#34;success\u0026#34;) :data{ data }, code{ code }, reason{ reason } { } friend std::ostream\u0026amp; operator\u0026lt;\u0026lt;(std::ostream\u0026amp; os, const Result\u0026lt; std::enable_if_t\u0026lt; !std::is_void\u0026lt;T\u0026gt;::value, T\u0026gt;\u0026gt; result) { os \u0026lt;\u0026lt; \u0026#34;Result(data = \u0026#34; \u0026lt;\u0026lt; result.data \u0026lt;\u0026lt;\u0026#34;, code = \u0026#34; \u0026lt;\u0026lt; result.code \u0026lt;\u0026lt;\u0026#34;, reason = \u0026#34; \u0026lt;\u0026lt; result.reason \u0026lt;\u0026lt;\u0026#34;)\u0026#34; \u0026lt;\u0026lt; std::endl; return os; } }; 例中，第3行和第8行都用到了类型限定，其实我们只需要在构造函数是对T限定就可以了。当用void来实例化Result时，将无法通过编译。\n其他问题 #C++模板有两方面的问题要解决，一方面是本身模板相关的问题，而另一方面就是和其他特性一起工作。如C++11引入了右值引用，但是右值引用通过参数传递以后会造成引用坍缩，丢失其右值引用的性质，表现得像一般引用类型，为了解决这个问题，C++提供了std::move工具。这对于普通函数是没问题的，但是假如这是一个模板函数呢？C++同样提供了完美转发的解决方法。 所谓完美转发，就是让右值引用保持右值引用，左值引用也保持左值引用。它需要配合万能引用一起使用。万能引用和右值引用很相似，只不过万能引用类型是不确定的，在编译期才能确定。看下面的例子\ntemplate \u0026lt;typename T\u0026gt; void test(T\u0026amp;\u0026amp; p) { std::cout \u0026lt;\u0026lt; \u0026#34;p = \u0026#34; \u0026lt;\u0026lt; std::forward\u0026lt;T\u0026gt;(p) \u0026lt;\u0026lt; std::endl; } int main() { int a = 1; test(a); test(std::move(a)); return 0; } // 输出 // p = 1 // p = 1 T\u0026amp;\u0026amp;是万能引用，因为它类型不确定，然后通过std::forward\u0026lt;\u0026gt;转发参数。可以看到在8，9行，我们成功传递给test左值和右值，并且也成功得到了预期结果，不需要为右值单独写函数来处理。模板的这个功能极大简化了函数的设计，对于API的设计来说简直就是救星。 此外，函数模板还有重载的问题。通常来说普通函数的优先级会高于函数模板的优先级，函数模板之间越特殊的会优先匹配等等。这些问题随着对模板了解的深入，会慢慢出现，但是在学习初期没必要花费太多精力来了解这些特性，一切以实用为主。\n总结 #模板是C++中很大的一个课题，融合了类型系统，标准库，类等一系列的大课题。所以写出完美的模板代码需要首先对这些课题有较为完整的了解。其次由于模板对类型控制较为宽松，还需要开发者对模板的适用范围有全局的把控，禁止什么，对什么类型需要特殊化处理，都要考虑到位，稍不注意就会隐藏一个难以察觉的bug。 总之就是一句话，模板是常学常新，常用常新的，需要在实践中学习，又要在学习中实践的东西，祝大家每次都有新收获！\n参考资料 # type_traints ","date":"31 October 2022","permalink":"/en/post/%E7%8E%B0%E4%BB%A3C++%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%20%E6%A8%A1%E6%9D%BF.html","section":"Posts","summary":"\u003cblockquote\u003e\n\u003cp\u003e模板作为C++重要的特性，一直有着举足轻重的地位，是编写高度抽象代码的利器。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"什么是模板\" class=\"relative group\"\u003e什么是模板 \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#%e4%bb%80%e4%b9%88%e6%98%af%e6%a8%a1%e6%9d%bf\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h3\u003e\u003cp\u003e模板在现实生活中就是范例：把都一样的部分固定起来，把变动的部分空出来，使用时将两部分合起来组成有效的东西。如申请书，Word模板都是这种形式。C++中的模板也是如此，不过更明确的是C++中的模板，变动的部分是一个代指类型的东西，称之为泛型参数。\u003c/p\u003e","title":"现代C++学习指南 模板"},{"content":" 在[上一章](https://www.yuque.com/docs/share/adb5b1e4-f3c6-46fd-ba4b-4dabce9b4f2a?# 《现代C++学习指南-类型系统》)我们探讨了C++的类型系统，并提出了从低到高，又从高到低的学习思路，本文就是一篇从高到低的学习指南，希望能提供一种新的视角。\n什么是标准库 #编程语言一般分为两个部分，一部分是语法部分，如上一章的类型系统，另一部分则是用这套语法完成的预定义的工具集，如本文的主题——标准库。标准库是一堆我们写代码时直接可以用的代码，就像是我们提前写好的一样，不仅如此，标准库还是跨平台的，还是经过工业级测试的，所以标准库有着靠谱，安全的特点。 C++标准库包括很多方面，有类vector、string等,有对象std::cin，std::cout等，还有函数move，copy等，所以一般按功能来对它们分类\n容器类 算法类 智能指针 线程相关 其他 当然，这些还不是全部，标准库是在不断扩充和完善的，学习标准库的宗旨也应该是学习它们的使用场景，而不是深入用法。比如容器类中就有很多功能类似的类，不同的业务场景有不同的选择。通过对它们的了解，我们更容易写出高效，简洁的代码。\n容器类 #容器类就是帮助管理一组数据的类，根据实现方式的不同，分为有序列表，无序列表和映射。 有序列表中的有序是指，数据组保存在一块连续的内存区域里，可以通过插入时的索引直接定位到原数据。因为数据是按顺序存入的，所以中途假如需要删除或者新增数据，在操作位置右边的数据都需要移动，操作的代价就比较大。由此也可看出它们的优势是顺序插入和尾部修改，还有直接查找，这方面的代表就是array，vector。 array是对原始数组的封装，并且解决了传递数组变成指针这样的问题，但是缺点是它的大小是固定的，适合用在数据量已知的情况。而vector又是对array的增强，不仅能完成所有array的操作，并且大小可变，所以绝大部分情况下，选择vector都是理想的选择。 无序列表的元素是单独存储的，相互之间用指针来查找相邻元素，由于指针可以轻易修改指向的指，所以对相邻元素的修改就变得很快捷。同样的道理，查找相邻元素只能靠指针跳转，查找某个值需要从一个指针开始查找，一次跳转一条数据，直到找到目标或者没有数据为止。所以无序列表的优势是快速地删除和插入新数据，不适合查找，其代表有list，forward_list。显然，有序列表和无序列表是互补的，我们在实际项目中，应该根据数据的操作来确定选择哪种容器。 映射则融合了有序列表和无序列表的优点，既可以快速插入和删除，又可以快速查找。为了满足各种使用场景，C++提供了map，multimap，unordered_map，unordered_multimap。从名字上就能看出来它们的差别。为了直观，我直接列了一个表\n是否排序 是否支持相同值 速度 unordered_map ❌ ❌ ❤️❤️❤️❤️ map ✅ ❌ ❤️❤️ multimap ✅ ✅ ❤️ unordered_multimap ✅ ✅ ❤️❤️❤️ 映射存储的是两个值，不同的类型实现方式不一样。由于map是需要排序的，所以通常它的实现是一种平衡二叉树，键就是它排序的依据。 而unordered_map是不需要排序的，所以它的实现通常是哈希表，即根据哈希函数的确定索引位置继而确定存储位置。 综上，容器类提供了一种操作多个同类型数据的接口，开发者通过对容器类方法的调用，可以实现对容器内数据的增删改查。大部分情况下，vector都是靠谱的选择，它提供了全功能的数据操作接口，支持动态长度，索引查询，并且简单高效。如果需要频繁地插入或者删除操作，也可以考虑list或者forward_list。map可以让数据保持有序，需要更快的速度而不是排序的话unorderer_map是更好的选择，如果相同值会出现多次就可以使用对应的multi版本。另外容器类也是很好的数据结构学习资源，C++的容器类几乎提供了数据结构中所有的形式，对数据结构越熟悉选择的容器类就越完美。\n算法 #之所以将算法放在容器类后面，是因为算法大部分是对容器类操作的加强，算法都定义在algorithm文件头里。这些算法都是短小精悍的，可以大大增加代码可读性，并且妥善处理了很多容易遗忘的边界问题。功能上可以分为增删改查几种操作，可以在实际有需要的时候在查看文档，具体可以参阅这里\n智能指针 #很早以前，我对智能指针的态度不是很好。因为刚开始学习C++时我就知道，不能单独使用指针，要把指针封装在类里，利用类的构造函数和析构函数管理指针，也就是RAII。最开始我以为这就够了,直到我遇到下面这种情况\npublic: Ptr():p{ new int } {} ~Ptr() { delete p; } int\u0026amp; get() { return *p; } void set(const int value) { *p = value; } private: int* p; }; void use(Ptr p) { //传进来的是复制构造出来的p\u0026#39;,函数返回后p\u0026#39;被销毁啦，两个指针指向的地址被回收，外面的p指针成为了野指针 } int main() { Ptr p; p.set(1); use(p); //p按值传递，调用了Ptr的复制构造函数，构造出了新对象p\u0026#39;,它的指针和p的指针指向同一个地方 std::cout \u0026lt;\u0026lt; p.get() \u0026lt;\u0026lt; std::endl; //p已经被销毁了，访问p的地址非法 return 0; } 调用use时，变量p被拷贝，也就出现了两个指针同时指向一块内存地址的情况。use函数执行完后，它的参数p被回收。也就是调用了Ptr的析构函数，也就是两个指针指向的地址被回收。所以24行调用get读取那个已经被回收了的地址就是非法操作，程序崩溃。 这可能是新手比较常遇到的一个问题，当然，解决这个问题也很简单，还用不到智能指针，只需要将函数use的参数改为引用类型就可以了，因为引用只是别名，不会产生新的指针，这也是我在类型系统篇中极力推荐引用为首选参数类型的原因之一。对于此例，数据不大，直接重写复制构造函数，重新申请一块内存也是一种思路。 此例中用到Ptr的地方只有一个，实际项目中Ptr往往需要用到很多次，我们不能保证不会出现忘记使用引用类型的情况，这种情况下重新申请内存也不适用，所以这个时候就需要智能指针来帮忙了。 现在思考另一种情况，某些操作我们不得不暴露出我们的指针供外部使用，随着业务的嵌套和调用链增加，很多时候会忘记或者不确定在什么时候调用delete释放内存。这也是用智能指针的一个场景。以上两种情况都是需要分享指针，对应智能指针中的shared_ptr。 shared_ptr顾名思义，它可以帮助开发者完成指针共享的问题，并且完美解决提前释放，不知何时释放，谁负责释放的问题。它的对应关系是一对多，一个实际的内存可以被多个shared_ptr共享 另外一种场景是我们希望自始至终某个指针某个时刻只属于一个对象，外部想要使用它要么通过拥有该指针的对象方法，要么把指针的所有权转移到自己身上，这种场景对应智能指针中的unique_ptr。 unique_ptr的对应关系是一对一，无论哪个时刻，只能有一个管理者拥有指针，也就只能由它负责释放了。假如想转移这种对应关系，只能通过std::move操作，不过这个操作之后，原先对象的指针就失效了，它也不再负责管理，所有的任务移交给了新的对象。这种特性特别适合资源敏感型的应用。\n线程库 #除了内存，线程是开发中另一个重要的课题。线程的难点在于不仅要管理线程对象，还要管理线程对象管理的资源，并且保证线程间数据同步。当然标准库已经做得足够好了，我们需要理解的是使用场景的问题。线程库主要包括线程对象thread，条件对象condition_variable，锁对象mutex。 使用thread可以很方便地把程序写成多线程，只需要三步：\nvoid plus(int a,int b){ //第一步：定义线程中要运行的函数 std::cout\u0026lt;\u0026lt;\u0026#34;running at sub thread\u0026#34;\u0026lt;\u0026lt;std::endl; std::cout\u0026lt;\u0026lt;\u0026#34;a + b = \u0026#34;\u0026lt;\u0026lt;a+b\u0026lt;\u0026lt;std::endl; } int main(){ std::thread thread{plus,1,1}; //第二步，定义std::thread对象，将函数作为参数 std::cout\u0026lt;\u0026lt;\u0026#34;continue running at main thread\u0026#34;\u0026lt;\u0026lt;std::endl; thread.join(); //第三步调用线程对象的join函数或者detach函数 std::cout\u0026lt;\u0026lt;\u0026#34;sub thread finished!\u0026#34;\u0026lt;\u0026lt;std::endl; } //输出 //\tcontinue running at main thread //\trunning at sub thread // a + b = 2 // sub thread finished! 难点在线程间通信，也就是解决两个问题\n线程1更新了变量v的值 线程2马上能读取到正确的变量v的值，即线程1更新的那个最新值 为了协调这两个过程，就出现了锁对象mutex和条件对象condition_variable。锁对象mutex保证变量按照正确的顺序更改。条件对象condition_variable保证更改能被其他线程监听到。\nint a,b; bool ready = false; std::mutex mux; std::condition_variable con; void plus() { std::cout \u0026lt;\u0026lt; \u0026#34;running at sub thread\u0026#34; \u0026lt;\u0026lt; std::endl; //因为我们要读取ready的最新值，所以要用锁保证读取结果的有效性 std::unique_lock\u0026lt;std::mutex\u0026gt; guard{ mux }; if (!ready) { //数据没准备好，休息一下！ con.wait(guard); } //这里就可以正确读变量a,b了 std::cout \u0026lt;\u0026lt; \u0026#34;a + b =\u0026#34; \u0026lt;\u0026lt; a + b \u0026lt;\u0026lt; std::endl; } int main() { std::thread thread{ plus}; std::cout \u0026lt;\u0026lt; \u0026#34;continue running at main thread\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;input a = \u0026#34;; std::cin \u0026gt;\u0026gt; a; std::cout \u0026lt;\u0026lt; \u0026#34;input b = \u0026#34;; std::cin \u0026gt;\u0026gt; b; { //数据准备好了，该通知子线程干活了，用大括号是因为想让锁因为guard的销毁即使释放，从未保证plus里面能重新获得锁 std::unique_lock\u0026lt;std::mutex\u0026gt; guard{ mux }; //更新数据 ready = true; //通知 con.notify_all(); } thread.join(); std::cout \u0026lt;\u0026lt; \u0026#34;sub thread finished!\u0026#34; \u0026lt;\u0026lt; std::endl; } 多线程另一个需要注意的问题就是死锁。死锁的前提是有两个锁\n线程1得到了锁a，还想得锁b 线程2得到了锁b，还想得锁a 然后，再加上一个前提：某一时刻，只有一个线程能拥有某个锁，就不难得出以下结论：线程a，b除非某一个放弃已得的锁，不然两个线程都会因为没得到需要的锁而一直死等，形成死锁。同时解决死锁的思路也呼之欲出：既然一个得了a，一个得了b，而锁同一时间只能被一个线程得到，那么所有线程都按先得a，再得b的顺序来就不会有锁被占用的问题了。另一个思路则可以从放弃上入手，既然都得不到，那么接下来的任务也做不了，不如直接放弃已经得到的，所以可以考虑使用timed_mutex。\n其他 #还有很多常用的库，如字符串string，时间chrono，还有在定义函数变量时常用的functional,异常exception，更多的内容可以在cplusplus找的参考。\n总结 #总的来说，标准库提供了一个展现C++语言能力的平台：帮助开发者更好更快完成开发任务的同时，还能启迪开发者实现更好的抽象和实践。如我就从标准库中学到了更规范地定义函数参数，更好的封装，以及其他好的思路。学习标准库不仅更好地掌握了语言本身，还掌握了更全面地分析问题，解决问题的方法，是值得花费一段时间学习的。 容器类是几乎所有项目都会用到的，也是比较好掌握的，主要可以从数据结构方面对照学习；智能指针则是处理指针问题的好帮手；线程相关的库是比较难掌握的，关键是要想明白使用场景和极端情况下的边界问题。很多时候边界问题可能不那么直观。如线程要求获得锁的情况就分为：锁空闲，锁被其他线程占有，锁被自己占有。不同的边界对于不同的锁，预期结果也是不同的，只有在明确场景的情况下，才能更好地理清锁的关系，从而解决好问题。 最好的学习还是在实践中主动使用。对于我，通常在遇到新问题的时候会先查查标准库有没有相应的库，有的话就是学习这个库的好时机。可以先概览库的定义和解决的问题，然后分析它提供的类，函数，对象等，再将自己的理解转换为项目中的代码，最后在实际效果中检验和修正想法，完成库的学习。\n","date":"20 July 2022","permalink":"/en/post/%E7%8E%B0%E4%BB%A3C++%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97-%E6%A0%87%E5%87%86%E5%BA%93.html","section":"Posts","summary":"\u003cblockquote\u003e\n\u003cp\u003e在[上一章](\u003ca href=\"https://www.yuque.com/docs/share/adb5b1e4-f3c6-46fd-ba4b-4dabce9b4f2a?#\" target=\"_blank\" rel=\"noreferrer\"\u003ehttps://www.yuque.com/docs/share/adb5b1e4-f3c6-46fd-ba4b-4dabce9b4f2a?#\u003c/a\u003e 《现代C++学习指南-类型系统》)我们探讨了C++的类型系统，并提出了从低到高，又从高到低的学习思路，本文就是一篇从高到低的学习指南，希望能提供一种新的视角。\u003c/p\u003e\n\u003c/blockquote\u003e","title":"现代C++学习指南-标准库"},{"content":" In the previous post, we provided a directional guide, but what and how to learn was not developed in detail. This article will build on the previous article and focus on how to learn C++\u0026rsquo;s type system.\nWrite in the front #Before we get into the type system, there is one thing we should agree on - use the modern syntax of C++ whenever possible. It is well known that many syntaxes in C++ are legal for compatibility reasons. But as new versions are released, some syntax may not be recommended or needs to be avoided. So this post also uses the recommended form of syntax (based on C++11 or above) whenever possible, which is what the title Modern C++ means. \u0026lt;! \u0026mdash;more\u0026ndash;\u0026gt; There are two benefits to using modern syntax. One, modern syntax can compile faster and more robust code. Compilers evolve as languages evolve, and modern syntaxes can help compilers do better optimization to some extent. Second, modern syntaxes are often more concise, intuitive, and uniform, which helps increase readability and maintainability. With that clear, let\u0026rsquo;s get our foot in the door of modern C++.\nType system #A program is a computational tool that produces the result of a calculation based on input, and a predefined method of computation. When the program is run, all three need to be represented in memory as appropriate values for the program to work properly, and the set of tools responsible for interpreting this is the type system. Numbers, strings, keyboard and mouse events, etc. are all data and actually exist in memory in the same form, but are treated differently when viewed through our human eyes. Numbers can be added, subtracted, multiplied, divided, and other arithmetic operations, but arithmetic operations on strings are meaningless, and keyboard and mouse values are usually just read, not calculated. It is because of these differences that the first task of a programming language is the need to define a type system that tells the computer what to do with the data in memory. In order to make programming languages as simple as possible, programming languages generally implement the type system in two steps, one for the compiler and the other for the types. The compiler part is responsible for interpreting the developer\u0026rsquo;s code into a suitable form so that it can be efficiently and accurately represented in memory. The type part defines the types that the compiler can handle, so that the developer can find the right data to accomplish the representation of inputs and outputs and the description of computational methods. The two complement and fulfill each other. The importance of types as an important representation of the type system in a programming language cannot be overstated. If you think of writing a program as building blocks, then the blocks of the program are the type system. The type system is the smallest unit that the developer can manipulate, it limits the rules of operation, but provides unlimited possibilities. c++ has a type system that is more flexible than the building blocks.\nType #:::info Types are the smallest unit of a programming language, and any sentence of code is a form of memory usage. ::: Talking about types in C++ also brings us to its three types of representation - normal types, pointers, and references. They are three different forms of memory usage and interpretation, and are the most basic forms of C++. Unlike most programming languages, C++ does not privilege built-in types, and all types can have a consistent syntax (via operator overloading) if the developer wants them to, so the following examples of types are suitable for all types. Common types are types without modifiers, such as int, long, double, etc. They are passed by value, i.e., assignments and function passes make a copy of the value. They are passed by value, which means that assignments and function passes make a copy of the value, and operations on the copied value do not affect the old value any further.\nint a=1; //old value, exists at address 1 int b=a; // new value, exists at address 2 b=2; //change new value, change address 2 // At this point a is still 1 and b becomes 2 `` ! [](https://cdn.nlark.com/yuque/0/2022/jpeg/29458474/1656843295159-ac2e516f-5226-4a08-9b72-d38d138627f6.jpeg) So what if we need to modify the old value, there are two ways to do this, one is a pointer and the other is a reference. Pointers are the magic inside C/C++, everything is a pointer. A pointer has two aspects, on the one hand it refers to a block of memory and on the other hand it can refer to the operations that are allowed to be performed on that block of memory. The value of a pointer is a block of memory address, and by manipulating a pointer, you manipulate the block of address it points to. ```cpp int a=1; //old value, exists at address 1 int* b=\u0026amp;a; //\u0026amp; stands for fetch address, read from right to left, fetch address of a - address 1, exist address 2 *b=2; //* is dereferencing, meaning take out the value that exists at address 2(b) and change the value at that address (address 1) to 2 // At this point a, *b becomes 2 `` ! [](https://cdn.nlark.com/yuque/0/2022/jpeg/29458474/1656842930791-9b5ed632-8d54-466a-960e-dfb58f5609c5.jpeg) References are an improved version of pointers. References avoid invalid references, but they cannot be reset and lack a certain degree of flexibility over pointers. ```cpp int a=1; //old value, exists at address 1 int\u0026amp; b=a; //\u0026amp; appears at the position of the variable declaration, which means that the variable is a reference variable, and reference variables must be initialized at the time of declaration b=2; // you can manipulate a reference variable like a normal variable, and at the same time, operations on it will be reflected on the original object // At this point a, b becomes 2 `` ### Variable definitions A type is just a syntactic definition, and to actually use that definition, we need to define variables in terms of types, i.e. variable definitions. C++ variable definitions are of the following form: ```cpp type name[{initial_value}] `` The key here is `type`. `type` is a combination of type and qualifier. Look at the following example: ```cpp int a; // Ordinary integer int* b; //The type is a combination of int and *, forming an integer pointer const int* c; //reading from right to left, * is a pointer and const int is a constant integer, making up the type of pointer to a constant integer int *const d; // also read from right to left, const is constant, followed by a pointer, indicating that this pointer is a constant pointer to the leftmost int, which consists of constant pointers to integers int\u0026amp; e=a; //The type is a combination of int and \u0026amp;, forming an integer reference constexpr int f=a+e; //constexpr means that this variable needs to be evaluated at compile time and is no longer mutable. `` Above, basically all the forms of variable definition, the type determines the basic attributes of the variable, and the qualifier limits the scope of the variable\u0026#39;s use. Defining a variable follows this same procedure, **first determining what type of variable we need, and secondly further determining if we need to add a qualifier to that variable, which many times is required**. The following steps can be followed to determine what kind of qualifier to add: 1. is a large object, consider declaring the variable as a reference type. Usually reference types are a better choice than pointer types. 1. Large objects may need to be reset, consider declaring them as pointers. 1. Just want a constant, add `constexpr`. 1. To read only this variable, add `const`. ### Variable initialization Variable definitions are often accompanied by initialization, which is important for local variables because the initial value of a local variable is uncertain, and using a variable without effectively initializing it can lead to uncontrollable problems. So strictly speaking, the preceding variable definition is not entirely correct. C++11 introduced a new, unified way of initializing variables, where the variable name is followed by curly braces that enclose the initialized value. This approach can be used for any variable and is called uniform initialization, as in: ```cpp int a{9527}; // common type string b={\u0026#34;abc\u0026#34;}; //another way to write, equivalent but not recommended Student c{\u0026#34;ZhangSan\u0026#34;, \u0026#34;20220226\u0026#34;,18}; //in curly brackets are constructor parameters `` Of course, in addition to defining a variable by its type name, you can also combine definition and initialization into the cleanest form below: ```cpp auto a={1}; // deduce to integer auto b=string{\u0026#34;abc\u0026#34;}; auto c=Student{\u0026#34;Zhang San\u0026#34;, \u0026#34;20220226\u0026#34;,18} `` Here `auto` means to let the compiler determine the type itself. The above is written in a way that makes full use of C++\u0026#39;s type derivation, which is the recommended form for many modern languages. Note, however, that `=` cannot be omitted when type derivation is used. Once we have initialized variables, we can use them for a variety of computational tasks.C++ implements a lot of built-in computational support for developers. C++ implements a lot of built-in computational support for the developer, such as addition, subtraction, multiplication and division of numbers, indexing of arrays, pointer manipulation, etc. There are also branching `if`, `switch`, looping `while`, `for`, etc. statements which provide us with more flexibility. ### Functions Variables are the smallest unit in a programming language, and as the complexity of a business increases, there are times when intermediate computations distract from the logic of the business and add complexity. In order to better organize the code, the type system adds functions to solve this problem. A function is also a type, a composite type. Its type consists of a combination of argument list and return value, which means that two functions, if they have the same argument list and return value, are equivalent from the compiler\u0026#39;s point of view. Of course, they are not enough, otherwise how can there be two parameter list and return value of the same function. A complete function also needs to have a function body and function name. So a function is usually of the following form: ```cpp // Regular function form [constexpr] return value Function name (argument list) [noexcept]{ function body (math.) } // Return value in postfixed form auto function name (argument list) -\u0026gt; return value `` When a function does not have a body, we usually call it a function declaration. Adding a function body is a function definition. ```cpp void f(int); //function declaration void fun(int value){ // function definition as represented by curly braces function body } `` This is the basic framework of a function, so let\u0026#39;s take a look at each of the parts that make it up. First of all, the simplest function name, ** it is actually a function of this type of a variable, the value of this variable represents a block of code starting from a certain location in the memory address **. As I said earlier, the reason why there can be two parameter lists and return values are the same function, but the compiler can identify, the main credit in the function name, so the function name is also the same as the name of the variable, is a kind of identifier. Then if the reverse, the same function name, but the parameter list or return value is different, this situation has a term - function overloading. Based on the understanding that functions are composite types, it is considered overloading if only one of them is different. In addition, in C++11, there is another kind of function without a name, called a lambda expression. lambda expressions are a kind of function value that is similar to a direct quantity, like 13, \u0026#39;c\u0026#39;, which is a kind of function that is not defined ahead of time, but is defined and used directly at the caller. The parameter list is an upgrade from the previous type definitions. All of what was said earlier about variable definitions applies to it, all three forms of variable definitions, multiple variables, variable initialization, etc. However, they all have new terms. Variables with a list of parameters are called formal parameters and initialization is called default parameters. Similarly formal parameters need to be initialized when they are actually used, but the initialization comes from the caller. Formal parameters without defaults need to be supplied at the time of calling and those with defaults can be omitted. ```cpp int plus(int a,int b=1){ //b is a default parameter return a+b; } int main(void){ int c=plus(1); // no value for b is provided, so b is initialized to 1, resulting in 2 int d=plus(2,2); //a,b are initialized to 2, resulting in 4 //int f=plus(1,2,3); //plus has only two formal parameters, i.e. two variables, so it can\u0026#39;t hold three values, so it compiles incorrectly. return 0; } `` Like the argument list, the return value is a variable that is returned to the caller via a `return` statement, so in terms of memory manipulation, it is an assignment operation. ```cpp std::string msg(){ std::string input; std::cin\u0026gt;\u0026gt;input; return input. } int main(void){ auto a=msg(); std::string b=msg();//the input returned by msg is copied into b return 0; } `` Unfortunately, C++ only supports a single return value, that is, a function call can only return a maximum of one value, if there is more than one value can only be returned in the form of a formal parameter, this way for the function call is not very friendly, so C++ proposed a new solution. ### Classes As the complexity of the business increases again, the number of formal parameters of a function may increase, or it may be necessary to return multiple values that are then passed between several different functions. This can lead to easily misplaced data and increased learning costs for the user. To solve these problems, engineers came up with object-oriented - multiple data packaging techniques. Expressed at the language level, it is ** using classes to package together a set of operations and the data needed to complete this set of operations **. Data as a class attribute, operation as a class method, the user through the method to operate the internal data, data no longer need to pass the user, management. This is undoubtedly for the developer is greatly simplified operation. We call this object-oriented programming, and the way to pass data between functions is called procedure-oriented programming. The underlying logic of these two ways is in fact the same, the transfer of parameters and function calls are not less, but the difference between object-oriented is that these cumbersome, error-prone work to the compiler to do, the developer only needs to do a good job in accordance with the rules of the design of the object-oriented work on it, the rest to the compiler. At this point, we have moved up one level in our type system. Classes are not only aggregates of multiple types, they are also aggregates of multiple functions, a higher level of abstraction than functions. You can see the following code comparison between procedural and object-oriented programming ```cpp struct Computer{ bool booted; friend std::ostream\u0026amp; operator\u0026lt;\u0026lt;(std::ostream\u0026amp; os,const Computer \u0026amp; c){ os\u0026lt;\u0026lt;\u0026#34;Computing\u0026#34;; return os. } }; void boot(Computer\u0026amp; c){ c.booted=true; std::cout\u0026lt;\u0026lt;\u0026#34;Booting...\u0026#34; ; } void compute(const Computer\u0026amp; c){ if(c.booted){ std::cout\u0026lt;\u0026lt;\u0026#34;Compute with\u0026#34;\u0026lt;\u0026lt;c; } } void shutdown(Computer\u0026amp; c){ c.booted=false; std::cout\u0026lt;\u0026lt;\u0026#34;Shutdown...\u0026#34; ; } int main(void){ auto c=Computer(); boot(c). compute(c). shutdown(c). return 0; } `` The most significant manifestation of procedural orientation is that the developer needs to pass data between functions and maintain the data state, which in the above example is `c`. ```cpp struct Computer{ bool booted; friend std::ostream\u0026amp; operator\u0026lt;\u0026lt;(std::ostream\u0026amp; os,const Computer \u0026amp; c){ os\u0026lt;\u0026lt;\u0026#34;Computing\u0026#34;; return os. } void boot(){ booted=true; std::cout\u0026lt;\u0026lt;\u0026#34;Booting...\u0026#34; ; } void compute(){ if(booted){ std::cout\u0026lt;\u0026lt;\u0026#34;Compute with\u0026#34;\u0026lt;\u0026lt;this; } } void shutdown(){ booted=false; std::cout\u0026lt;\u0026lt;\u0026#34;Shutdown...\u0026#34; ; } }; int main(void){ auto c=Computer(); c.boot(); c.compute(); c.shutdown(); return 0; } `` The most significant change that can be seen in object-oriented code is that methods have fewer parameters, but the data defined by the class can be accessed directly inside the method. Another change occurs on the calling side. Instead of passing data to the method, the caller calls the method with data. This is the essence of object-orientation - it is data-centric. Of course, the encapsulation function of the class is only a small part of the class function, we will cover more class knowledge later. As a beginner, we understand this step will be able to read most of the code. ### Summary The type system is the basic component of a language, which supports the high-level functions of the whole system, and many high-level features are evolved on the basis of the type system. Therefore, learning the type system of a language is a process from low to high, and from high to low. Starting from the most basic types, we learn how to construct high-level types from the low-level types, and then stand on the height of high-level types, and examine how the high-level types are constructed from the low-level types. This up and down, high and low basically makes most of the features of the language clear. Low-level types are more oriented towards making the compiler work better, and high-level types are more oriented towards making the developer work better. C++ provides various levels of support from common types, functions, and classes, which gives the developer more freedom of choice, and of course makes it more difficult for the developer to learn. But developers don\u0026#39;t always need all the choices, so I think the right learning should be guided by the size of the project. Some projects, which don\u0026#39;t use object orientation at all, can focus on building function sets that work well. And there are projects where object orientation is a good choice, and time needs to be spent on classes. Going back to the building blocks example at the beginning, the choice of building blocks depends entirely on what we want to model, and if we don\u0026#39;t have the right ones, we can create our own. That\u0026#39;s the beauty of C++. ","date":"26 June 2022","permalink":"/en/post/modern-c++-study-guide-type-system.html","section":"Posts","summary":"In the previous post, we provided a directional guide, but what and how to learn was not developed in detail.","title":"Modern C++ Study Guide - Type Systems"},{"content":"","date":null,"permalink":"/en/tags/study-guides/","section":"Tags","summary":"","title":"Study guides"},{"content":"在Android开发中,有时候出于安全，性能，代码共用的考虑，需要使用C/C++编写的库。虽然在现代化工具链的支持下，这个工作的难度已经大大降低，但是毕竟万事开头难，初学者往往还是会遇到很多不可预测的问题。本篇就是基于此背景下写的一份简陋指南，希望能对刚开始编写C/C++库的读者有所帮助。同时为了尽可能减少认知断层，本篇将试着从一个最简单的功能开始，逐步添加工具链，直到实现最终功能，真正做到知其然且之所以然。\n目标 #本篇的目标很简单，就是能在Android应用中调用到C/C++的函数——接收两个整型值，返回两者相加后的值，暂定这个函数为plus。\n从C++源文件开始 #为了从我们最熟悉的地方开始,我们先不用复杂工具,先从最原始的C++源文件开始.\n打开你喜欢的任何一个文本编辑器，VS Code，Notpad++，记事本都行，新建一个文本文件，并另存为math.cpp。接下来,就可以在这个文件中编写代码了.\n前面我们的目标已经说得很清楚,实现个plus函数，接收两个整型值，返回两者之和，所以它可能是下面这样\nint plus(int left,int right) { return left + right; } 我们的源文件就这样完成了，是不是很简单。\n但是仅仅有源文件是不够的，因为这个只是给人看的，机器看不懂。所以我们就需要第一个工具——编译器。编译器能帮我们把人看得懂的转化成机器也能看得懂的东西。\n编译器 #编译器是个复杂工程，但是都是服务于两个基本功能\n理解源文件的内容（人能看懂的）——检查出源文件中的语法错误 理解二进制的内容（机器能看懂的）——生成二进制的机器码。 基于这两个朴素的功能，编译器却是挠断了头。难点在于功能2。基于这个难点编译器分成了很多种，常见的像Windows平台的VS，Linux平台的G++,Apple的Clang。而对于Android来说，情况略有不同，前面这些编译器都是运行在特定系统上的，编译出来的程序通常也只能运行在对应的系统上。以我现在的机器为例，我现在是在Deepin上写的C++代码，但是我们的目标是让代码跑在Android手机上，是两个不同的平台。更悲观的是，目前为止，还没有一款可以在手机上运行的编译器。那我们是不是就不能在手机上运行C++代码了？当然不是，因为有交叉编译。\n交叉编译就是在一个平台上将代码生成另一个平台可执行对象的技术。它和普通编译最大的不同是在链接上。因为一般的链接直接可以去系统库找到合适的库文件，而交叉编译不行，因为当前的平台不是最终运行代码的平台。所以交叉编译还需要有目标平台的常用库。当然，这些Google都替我们准备好了，称为NDK。\nNDK #NDK全称是Native Development Kit，里面有很多工具，编译器，链接器，标准库，共享库。这些都是交叉编译必不可少的部分。为了理解方便，我们首先来看看它的文件结构。以我这台机器上的版本为例——/home/Andy/Android/Sdk/ndk/21.4.7075529（Windows上默认位置则是c:\\Users\\xxx\\AppData\\Local\\Android\\Sdk\\）。 NDK就保存在Sdk目录下，以ndk命名，并且使用版本号作为该版本的根目录，如示例中，我安装的NDK版本就是21.4.7075529。同时该示例还是ANDROID_NDK这个环境变量的值。也就是说，在确定环境变量前，我们需要先确定选用的NDK版本，并且路径的值取到版本号目录。\n了解了它的存储位置，接下来我们需要认识两个重要的目录\nbuild/cmake/，这个文件夹，稍后我们再展开。 toolchains/llvm/prebuild/linux-x86_64，最后的linux-x86_64根据平台不同，名称也不同，如Windows平台上就是以Windows开头，但是一般不会找错，因为这个路径下就一个文件夹，并且前面都是一样的。这里有我们心心念念的编译器，链接器，库，文件头等。如编译器就存在这个路径下的bin目录里，它们都是以clang和clang++结尾的，如aarch64-linux-android21-clang++ aarch64代表着这个编译器能生成用在arm64架构机器上的二进制文件，其他对应的还有armv7a，x86_64等。不同的平台要使用相匹配的编译器。它就是交叉编译中所说的目标平台。\nlinux代表我们执行编译这个操作发生在linux机器上，它就是交叉编译中所说的主机平台。\nandroid21这个显然就是目标系统版本了\nclang++代表它是个C++编译器，对应的C编译器是clang。\n可以看到，对于Android来说，不同的主机，不同的指令集，不同的Android版本，都对应着一个编译器。 了解了这么多，终于到激动人性的时刻啦，接下来，我们来编译一下前面的C++文件看看。\n编译 #通过aarch64-linux-android21-clang++ --help查看参数，会发现它有很多参数和选项，现在我们只想验证下我们的C++源文件有没有语法错误，所以就不管那些复杂的东西，直接一个aarch64-linux-android21-clang++ -c math.cpp执行编译。\n命令执行完后，假如一切顺利，就会在math.cpp相同目录下生成math.o对象文件，说明我们的源码没有语法错误，可进行到下一步的链接。\n不过，在此之前，先打断一下。通常我们的项目会包含很多源文件，引用一些第三方库，每次都用手工的形式编译，链接显然是低效且容易出错的。在工具已经很成熟的现在，我们应该尽量使用成熟的工具，将重心放在我们的业务逻辑上来，CMake就是这样的一个工具。\nCMake #CMake是个跨平台的项目构建工具。怎么理解呢？编写C++代码时，有时候需要引用其他目录的文件头，但是在编译阶段，编译器是不知道该去哪里查找文件头的，所以需要一种配置告诉编译器文件头的查找位置。再者，分布在不同目录的源码，需要根据一定的需求打包成不同的库。又或者，项目中引用了第三方库，需要在链接阶段告诉链接器从哪个位置查找库，种种这些都是需要配置的东西。\n而不同的系统，不同的IDE对于上述配置的支持是不尽相同的，如Windows上的Visual Studio就是需要在项目的属性里面配置。在开发者使用同样的工具时，问题还不是很大。但是一旦涉及到多平台，多IDE的情况，协同开发就会花费大把的时间在配置上。CMake就是为了解决这些问题应运而生的。\nCMake的配置信息都是写在名为CMakeLists.txt的文件中。如前面提到头文件引用，源码依赖，库依赖等等，只需要在CmakeLists.txt中写一次，就可以在Windows，MacOS，Linux平台上的主流IDE上无缝使用。如我在Windows的Visual Studio上创建了一个CMake的项目，配置好了依赖信息,传给同事。同事用MacOS开发，他可以在一点不修改的情况下，马上完成编译，打包，测试等工作。这就是CMake跨平台的威力——简洁，高效，灵活。\n使用CMake管理项目 #建CMake项目 #我们前面已经有了math.cpp，又有了CMake，现在就把他们结合一下。\n怎样建立一个CMake项目呢？一共分三步：\n建一个文件夹 示例中我们就建一个math的文件夹吧。\n在新建的文件夹里新建CMakeLists.txt文本文件。注意，这里的文件名不能变。\n在新建的CMakeLists.txt文件里配置项目信息。 最简单的CMake项目信息需要包括至少三个东西 1）、支持的最低CMake版本\ncmake_minimum_required(VERSION 3.18.1) 2）、项目名称\nproject(math) 3）、生成物——生成物可能是可执行文件，也可能是库。因为我们要生成Android上的库，所以这里是的生成物是库。\nadd_library(${PROJECT_NAME} SHARED math.cpp) 经过这三步，CMake项目就建成了。下一步我们来试试用CMake来编译项目。\n编译CMake项目 #在执行真正的编译前，CMake有个准备阶段，这个阶段CMake会收集必要的信息，然后生成满足条件的工程项目，然后才能执行编译。\n那么什么是必要的信息呢？CMake为了尽可能降低复杂性，会自己猜测收集一些信息。\n如我们在Windows上执行生成操作，CMake会默认目标平台就是Windows，默认生成VS的工程，所以在Windows上编译Windows上的库就几乎是零配置的。\n在math目录下新建一个build的目录，然后把工作目录切换到build目录。\ncd build cmake .. 在命令执行之后，就能在build目录下找到VS的工程，可以直接使用VS打开，无错误地完成编译。当然，更快的方法还是直接使用CMake编译.\n使用CMake编译\ncmake --build . 注意前面的..代表父目录，也就是CMakeLists.txt文件存在的math目录，而.则代表当前目录，即build这个目录。假如这两步都顺利执行了，我们就能在build目录下收获一个库文件。Windows平台上可能叫math.dll，而Linux平台上可能叫math.so，但是都是动态库，因为我们在CMakelists.txt文件里配置的就是动态库。\n从上面的流程来看，CMake的工作流程不复杂。但是我们使用的是默认配置，也就是最终生成的库只能用在编译的平台上。要使用CMake编译Android库，我们就需要在生成工程时，手动告诉CMake一些配置，而不是让CMake去猜。\nCMake的交叉编译 #配置参数从哪来 #虽然我们不知道完成交叉编译的最少配置是什么，但是我们可以猜一下。\n首先要完成源码的编译，编译器和链接器少不了，前面也知道了,Android平台上有专门的编译器和链接器，所以至少有个配置应该是告诉CMake用哪一个编译器和链接器。\n其次Android的系统版本和架构也是必不可少的，毕竟对于Android开发来说，这个对于Android应用都很重要。\n还能想到其他参数吗，好像想不到了。不过，好消息是，Google替我们想好了，那就是直接使用CMAKE——TOOLCHAIIIN_FILE。这个选项是CMake 提供的，使用的时候把配置文件路径设置为它的值就可以了，CMake会通过这个路径查找到目标文件，使用目标文件里面的配置代替它的自己靠猜的参数。而这个配置文件，就是刚才提到过的两个重要文件夹之一的build/camke,我们的配置文件就是该文件夹下面的android.toolchain.cmake。\nGoogle的CMake配置文件 #android.toolchain.cmake扮演了一个包装器的作用，它会利用提供给它的参数，和默认的配置，共同完成CMake的配置工作。其实这个文件还是个很好的CMake学习资料，可以学到很多CMake的技巧。现在，我们先不学CMake相关的，先来看看我们可用的参数有哪些。在文件的开头，Google就把可配置的参数都列举出来了\nANDROID_TOOLCHAIN ANDROID_ABI ANDROID_PLATFORM ANDROID_STL ANDROID_PIE ANDROID_CPP_FEATURES ANDROID_ALLOW_UNDEFINED_SYMBOLS ANDROID_ARM_MODE ANDROID_ARM_NEON ANDROID_DISABLE_FORMAT_STRING_CHECKS ANDROID_CCACHE 这些参数其实不是CMake的参数，在配置文件被执行的过程中，这些参数会被转换成真正的CMake参数。我们可以通过指定这些参数的值，让CMake完成不同的构建需求。假如都不指定，则会使用默认值，不同的NDK版本，默认值可能会不一样。\n我们来着重看看最关键的ANDROID_ABI和ANDROID_PLATFORM。前面这个是指当前构建的包运行的CPU指令集是哪一个，可选的值有arneabi-v7a，arn64-v8a，x86，x86_64，mips，mips64。后一个则是指构建包的Android版本。它的值有两种形式，一种就是直接android-[version]的形式[version]在使用时替换成具体的系统版本，如android-23，代表最低支持的系统版本是Android 23。另一种形式是字符串latest。这个值就如这个单词的意思一样，用最新的。\n那么我们怎么知道哪个参数可以取哪些值呢，有个简单方法：先在文件头确定要查看的参数，然后全局搜索，看set和if相关的语句就能确定它支持的参数形式了。\n使用配置文件完成交叉编译 #说了那么一大堆，回到最开始的例子上来。现在我们有了CMakelists.txt，还有了math.cpp，又找到了针对Android的配置文件android.toolchin.cmake。那么怎样才能把三者结合起来呢，这就不得不提到CMake的参数配置了。\n在前面，我们直接使用\ncmake .. 就完成了工程文件的生成配置，但是其实它是可以传递参数的。***CMake的参数都是以-D开头，用空白符分割的键值对。***而CMake缺省的参数都是以CMAKE为开头的，所以大部分情况下参数的形式都是-DCMAKE_XXX这种。如给CMake传递toolchain文件的形式就是\ncmake -DCMAKE_TOOLCHAIN_FILE=/home/Andy/Android/Sdk/ndk/21.4.7075529/build/cmake/android.toolchain.cmake 这个参数的意思就是告诉CMake，使用=后面指定的文件来配置CMake的参数。\n然而，完成交叉编译，我们还少一个选项——-G。这个选项是交叉编译必需的。因为交叉编译CMake不知道该生成什么形式的工程，所以需要使用这个选项指定生成工程的类型。一种是传统形式的Make工程，指定形式是\ncmake -G \u0026#34;Unix Makefiles\u0026#34; 可以看出，这种形式是基于Unix平台下的Make工程的，它使用make作为构建工具，所以指定这种形式以后，还需要指定make的路径，工程才能顺利完成编译。而另一种Google推荐的方式是Ninja，这种方式更简单，因为不需要单独指定Ninja的路径，它默认就随CMake安装在同一个目录下，所以可以减少一个传参。Ninja也是一种构建工具，但是专注速度，所以我们这一次就使用Ninja。它的指定方式是这样的\ncmake -G Ninja 结合以上两个参数，就可以得到最终的编译命令\ncmake -GNinja -DCMAKE_TOOLCHAIN_FILE=/home/Andy/Android/Sdk/ndk/21.4.7075529/build/cmake/android.toolchain.cmake .. 生成工程后再执行编译\ncmake --build . 我们就得到了最终能运行在Android上的动态库了。用我这个NDK版本编译出来的动态库支持的Android版本是21,指令集是armeabi-v7a。当然根据前面的描述我们可以像前面传递toolchain文件一下传递期望的参数，如以最新版的Android版本构建x86的库，就可以这样写\ncmake -GNinja -DCMAKE_TOOLCHAIN_FILE=/home/Andy/Android/Sdk/ndk/21.4.7075529/build/cmake/android.toolchain.cmake -DANDROID_PLATFORM=latest -DANDROID_ABI=x86 .. 这就给我们个思路，假如有些第三方库没有提供编译指南，但是是用CMake管理的，我们就可以直接套用上面的公式来编译这个第三方库。\nJNI #前面在CMake的帮助下，我们已经得到了libmath.so动态库,但是这个库还是不能被Android应用直接使用，因为Android应用是用Java（Kotlin）语言开发的，而它们都是JVM语言，代码都是跑在JVM上的。要想使用这个库，还需要想办法让库加载到JVM中，然后才有可能访问得到。它碰巧的是，JVM还真有这个能力，它就是JNI。\nJNI基本思想 #JNI能提供Java到C/C++的双向访问，也就是可以在Java代码里访问C/C++的方法或者数据，反过来也一样支持，这过程中JVM功不可没。所以要理解JNI技术，需要我们以JVM的角度思考问题。\nJVM好比一个货物集散中心，无论是去哪个地方的货物都需要先来到这个集散中心，再通过它把货物分发到目的地。这里的货物就可以是Java方法或者C/C++函数。但是和普通的快递不一样的是，这里的货物不知道自己的目的地是哪里，需要集散中心自己去找。那么找的依据从哪里来呢，也就是怎样保证集散中心查找结果的唯一性呢，最简单的方法当然就是货物自己标识自己，并且保证它的唯一性。\n显然对于Java来说，这个问题很好解决。Java有着层层保证唯一性的机制。\n包名可以保证类名的唯一性； 类名可以保证同一包名下类的唯一性； 同一个类下可以用方法名保证唯一性； 方法发生重载的时候可以用参数类型和个数确定类的唯一性。 而对于C/C++来说，没有包名和类名，那么用方法名和方法参数可以确定唯一性吗？答案是可以，只要我们把包名和类名作为一种限定条件。\n而添加限定条件的方式有两种，一种就是简单粗暴，直接把包名类名作为函数名的一部分，这样JVM也不用看其他的东西，直接粗暴地将包名，类名，函数名和参数这些对应起来就能确定对端对应的方法了。这种方法叫做静态注册。其实这和Android里面的广播特别像：广播的静态注册就是直接粗暴地在AndroidManifest文件中写死了，不用在代码里配置，一写了就生效。对应于静态注册，肯定还有个动态注册的方法。动态注册就是用写代码的方式告诉JVM函数间的对应关系，而不是让它在函数调用时再去查找。显然这种方式的优势就是调用速度更快一点，毕竟我们只需要一次注册，就可以在后续调用中直接访问到对端，不再需要查找操作。但是同样和Android中广播的动态注册一样，动态注册要繁琐得多，而且动态注册还要注意把握好注册时机，不然容易造成调用失败。我们继续以前面的libmath.so为例讲解。\nJava使用本地库 #Java端访问C/C++函数很简单，一共分三步：\nJava调用System.loadLibrary()方法载入库\nSystem.loadlibrary(\u0026#34;math.so\u0026#34;); 这里有个值得注意的地方，CMake生成的动态库是libmath.so，但是这里只写了math.so，也就是说不需要传递lib这个前缀。这一步执行完后，JVM就知道有个plus函数了。\nJava声明一个和C++函数对应的native方法。这里对应指的是参数列表和返回值要保持一致，方法名则可以不一致。\npublic native int nativePlus(int left,int right); 通常，习惯将native方法添加native的前缀。\n在需要的地方直接调用这个native方法。调用方法和普通的Java方法是一致的，传递匹配的参数，用匹配的类型接收返回值。\n把这几布融合到一个类里面就是这样\npackage hongui.me; import android.os.Bundle; import androidx.annotation.Nullable; import androidx.appcompat.app.AppCompatActivity; import hongui.me.databinding.ActivityMainBinding; public class MainActivity extends AppCompatActivity { static { System.loadLibrary(\u0026#34;me\u0026#34;); } ActivityMainBinding binding; private native int nativePlus(int left,int right); @Override protected void onCreate(@Nullable Bundle savedInstanceState) { super.onCreate(savedInstanceState); binding = ActivityMainBinding.inflate(getLayoutInflater()); setContentView(binding.getRoot()); // Example of a call to a native method binding.sampleText.setText(\u0026#34;1 + 1 = \u0026#34;+nativePlus(1,1)); } } C/C++端引入JNI #JNI其实对于C/C++来说是一层适配层，在这一层主要做函数转换的工作，不做具体的功能实现，所以，通常来说我们会新建一个源文件，用来专门处理JNI层的问题，而JNI层最主要的问题当然就是前面提到的方法注册问题了。\n静态注册 #静态注册的基本思路就是根据现有的Java native方法写一个与之对应的C/C++函数签名，具体来说分四步。\n先写出和Java native函数一模一样的函数签名 int nativePlus(int left,int right) 在函数名前面添加包名和类名。因为包名在Java中是用.分割的，而C/C++中点通常是用作函数调用，为了避免编译错误，需要把.替换成_。 hongui_me_MainActivity_nativePlus(int left,int right) 转换函数参数。前面提到过所有的操作都是基于JVM的，在Java中，这些是自然而然的，但是在C/C++中就没有JVM环境，提供JVM 环境的形式就只能是添加参数。为了达到这个目的，任何JNI的函数都要在参数列表开头添加两个参数。而Java里面的最小环境是线程，所以第一个参数就是代表调用这个函数时，调用方的线程环境对象JNIEnv，这个对象是C/C++访问Java的唯一通道。第二个则是调用对象。因为Java中不能直接调用方法，需要通过类名或者某个类来调用方法，第二个参数就代表那个对象或者那个类，它的类型是jobjet。从第三个参数开始，参数列表就和Java端一一对应了，但是也只是对应，毕竟有些类型在C/C++端是没有的，这就是JNI中的类型系统了，对于我们当前的例子来说Java里面的int值对应着JNI里面的jint,所以后两个参数都是jint类型。这一步至关重要，任何一个参数转换失败都可能造成程序崩溃。 hongui_me_MainActivity_nativePlus( JNIEnv* env, jobject /* this */, jint left, jint right) 添加必要前缀。这一步会很容易被忽略，因为这一部分不是那么自然而然。首先我们的函数名还得加一个前缀Java,现在的函数名变成了这样Java_hongui_me_MainActivity_nativePlus。其次在返回值两头需要添加JNIEXPORT和JNICALL，这里返回值是jint，所以添加完这两个宏之后是这样JNIEXPORT jint JNICALL。最后还要在最开头添加extern \u0026quot;C\u0026quot; 的兼容指令。至于为啥要添加这一步，感兴趣的读者可以去详细了解，简单概括就是这是JNI的规范。 经过这四步，最终静态方法找函数的C/C++函数签名变成了这样\n#include \u0026#34;math.h\u0026#34; extern \u0026#34;C\u0026#34; JNIEXPORT jint JNICALL Java_hongui_me_MainActivity_nativePlus( JNIEnv* env, jobject /* this */, jint left, jint right){ return plus(left,right); } 注意到，这里我把前面的math.cpp改成了math.h，并在JNI适配文件（文件名是native_jni.cpp）中调用了这个函数。所以现在有两个源文件了，需要更新一下CMakeList.txt。\ncmake_minimum_required(VERSION 3.18。1) project(math) add_library(${PROJECT_NAME} SHARED native_jni.cpp) 可以看到这里我们只把最后一行的文件名改了，因为CMakeLists.txt当前所在的目录也是include的查找目录，所以不需要给它单独设置值，假如需要添加其他位置的头文件则可以使用include_directories(dir)添加。\n现在使用CMake重新编译，生成动态库，这次Java就能直接不报错运行了。\n动态注册 #前面提到过动态注册需要注意注册时机，那么什么算是好时机呢？在前面Java使用本地库这一节，我们知道，要想使用库，必须先载入，载入成功后就可以调用JNI方法了。那么动态注册必然要发生在载入之后，使用之前。JNI很人性化的想到了这一点，在库载入完成以后会马上调用jint JNI_OnLoad(JavaVM *vm, void *reserved)这个函数，这个方法还提供了一个关键的JavaVM对象，简直就是动态注册的最佳入口了。确定了注册时机，现在我们来实操一下。***注意：动态注册和静态注册都是C/C++端实现JNI函数的一种方式，同一个函数一般只采用一种注册方式。***所以，接下来的步骤是和静态注册平行的，并不是先后关系。\n动态注册分六步\n新建native_jni.cpp文件，添加JNI_OnLoad()函数的实现。 extern \u0026#34;C\u0026#34; JNIEXPORT jint JNICALL JNI_OnLoad(JavaVM *vm, void *reserved) { return JNI_VERSION_1_6; } 这就是这个函数的标准形式和实现，前面那一串都是JNI函数的标准形式，关键点在于函数名和参数以及返回值。要想这个函数在库载入后自动调用，函数名必须是这个，而且参数形式也不能变，并且用最后的返回值告诉JVM当前JNI的版本。也就是说，这些都是模板，直接搬就行。\n得到JNIEnv对象 前面提到过，所有的JNI相关的操作都是通过JNIEnv对象完成的，但是现在我们只有个JavaVM对象，显然秘诀就在JavaVM身上。 通过它的GetEnv方法就可以得到JNIEnv对象\nJNIEnv *env = nullptr; vm-\u0026gt;GetEnv(env, JNI_VERSION_1_6); 找到目标类 前面说过，动态注册和静态注册都是要有包名和类名最限定的，只是使用方式不一样而已。所以动态注册我们也还是要使用到包名和类名，不过这次的形式又不一样了。静态注册包名类名用_代替.，这一次要用/代替.。所以我们最终的类形式是hongui/me/MainActivity。这是一个字符串形式，怎样将它转换成JNI中的jclass类型呢，这就该第二步的JNIEnv出场了。\njclass cls=env-\u0026gt;FindClass(\u0026#34;hongui/me/MainActivity\u0026#34;); 这个cls对象就和Java里面那个MainActivity是一一对应的了。有了类对象下一步当然就是方法了。\n生成JNI函数对象数组。 因为动态注册可以同时注册一个类的多个方法，所以注册参数是数组形式的，而数组的类型是JNINativeMethod。这个类型的作用就是把Java端的native方法和JNI方法联系在一起，怎么做的呢，看它结构。\ntypedef struct { const char* name; const char* signature; void* fnPtr; } JNINativeMethod; name对应Java端那个native的方法名，所以这个值应该是nativePlus。 signature对应着这个native方法的参数列表外加函数类型的签名。 什么是签名呢，就是类型简写。在Java中有八大基本类型，还有方法，对象，类。数组等，这些东西都有一套对应的字符串形式，好比是一张哈希表，键是类型的字符串表示，值是对应的Java类型。如jint是真正的JNI类型，它的类型签名是I，也就是int的首字母大写。\n函数也有自己的类型签名(paramType)returnType这里的paramType和returnType都需要是JNI类型签名，类型间不需要任何分隔符。\n综上，nativePlus的类型签名是(II)I。两个整型参数，返回另一个整型。\nfnPtr正如它名字一样，它是一个函数指针，值就是我们真正的nativePlus实现了（这里我们还没有实现，所以先假定是jni_plus）。 综上，最终函数对象数组应该是下面这样\nJNINativeMethod methods[] = { {\u0026#34;nativePlus\u0026#34;,\u0026#34;(II)I\u0026#34;,reinterpret_cast\u0026lt;void *\u0026gt;(jni_plus)} }; 注册 现在有了代表类的jclass对象，还有了代表方法的JNINativeMethod数组，还有JNIEnv对象，把它们结合起来就可以完成注册了\nenv-\u0026gt;RegisterNatives(cls,methods,sizeof(methods)/sizeof(methods[0])); 这里第三个参数是代表方法的个数，我们使用了sizeof操作法得出了所有的methods的大小，再用sizeof得出第一个元素的大小，就可以得到methods的个数。当然，这里直接手动填入1也是可以的。\n实现JNI函数 在第4步，我们用了个jni_plus来代表nativePlus的本地实现，但是这个函数实际上还没有创建，我们需要在源文件中定义。现在这个函数名就可以随便起了，不用像静态注册那样那么长还不能随便命名，只要保持最终的函数名和注册时用的那个名字一致就可以了。但是这里还是要加上extern \u0026quot;C\u0026quot;的前缀，避免编译器对函数名进行特殊处理。参数列表和静态注册完全一致。所以，我们最终的函数实现如下。\n#include \u0026#34;math.h\u0026#34; extern \u0026#34;C\u0026#34; jint jni_plus( JNIEnv* env, jobject /* this */, jint left, jint right){ return plus(left,right); } 好了，动态注册的实现形式也完成了，CMake编译后你会发现结果和静态注册完全一致。所以这两种注册方式完全取决于个人喜好和需求，当需要频繁调用native方法时，我觉得动态注册是有优势的，但是假如调用次数很少，完全可以直接用静态注册，查找消耗完全可以忽略不记。\nOne more thing #前面我提到CMake是管理C/C++项目的高手，但是对于Android开发来说，Gradle才是YYDS。这一点Google也意识到了，所以gradle的插件上直接提供了CMake和Gradle无缝衔接的丝滑配置。在android这个构建块下，可以直接配置CMakeLists.txt的路径和版本信息。\nexternalNativeBuild { cmake { path file(\u0026#39;src/main/cpp/CMakeLists.txt\u0026#39;) version \u0026#39;3.20.5\u0026#39; } } 这样，后面无论是修改了C/C++代码，还是修改了Java代码，都可以直接点击运行，gradle会帮助我们编译好相应的库并拷贝到最终目录里，完全不再需要我们手动编译和拷贝库文件了。当然假如你对它的默认行为还不满意，还可以通过defaultConfig配置默认行为，它的大概配置可以是这样\nandroid { compileSdkVersion 29 defaultConfig { minSdkVersion 21 targetSdkVersion 29 testInstrumentationRunner \u0026#34;androidx.test.runner.AndroidJUnitRunner\u0026#34; consumerProguardFiles \u0026#39;consumer-rules.pro\u0026#39; externalNativeBuild { cmake { cppFlags += \u0026#34;-std=c++1z\u0026#34; arguments \u0026#39;-DANDROID_STL=c++_shared\u0026#39; abiFilters \u0026#39;armeabi-v7a\u0026#39;, \u0026#39;arm64-v8a\u0026#39; } } } } 这里cppFlags是指定C++相关参数的，对应的还有个cFlags用来指定C相关参数。arguments则是指定CMake的编译参数，最后一个就是我们熟悉的库最终要编译生成几个架构包了，我们这里只是生成两个。\n有了这些配置，Android Studio开发NDK完全就像开发Java一样，都有智能提示，都可以即时编译，即时运行，纵享丝滑。\n总结 #NDK开发其实应该分为两部分，C++开发和JNI开发。 C++开发和PC上的C++开发完全一致，可以使用标准库，可以引用第三方库，随着项目规模的扩大，引入了CMake来管理项目，这对于跨平台项目来说优势明显，还可以无缝衔接到Gradle中。 而JNI开发则更多的是关注C/C++端和Java端的对应关系，每一个Java端的native方法都要有一个对应的C/C++函数与之对应，JNI提供 静态注册和动态注册两种方式来完成这一工作，但其核心都是利用包名，类名，函数名，参数列表来确定唯一性。静态注册将包名，类名体现在函数名上，动态注册则是使用类对象，本地方法对象，JNIENV的注册方法来实现唯一性。 NDK则是后面的大BOSS，它提供编译器，链接器等工具完成交叉编译，还有一些系统自带的库，如log,z,opengl等等供我们直接使用。\n","date":"6 March 2022","permalink":"/en/post/Android-NDk%E5%BC%80%E5%8F%91%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.html","section":"Posts","summary":"\u003cp\u003e在Android开发中,有时候出于安全，性能，代码共用的考虑，需要使用C/C++编写的库。虽然在现代化工具链的支持下，这个工作的难度已经大大降低，但是毕竟万事开头难，初学者往往还是会遇到很多不可预测的问题。本篇就是基于此背景下写的一份简陋指南，希望能对刚开始编写C/C++库的读者有所帮助。同时为了尽可能减少认知断层，本篇将试着从一个最简单的功能开始，逐步添加工具链，直到实现最终功能，真正做到知其然且之所以然。\u003c/p\u003e","title":"Android-NDK开发——基本概念"},{"content":"","date":null,"permalink":"/en/tags/jni/","section":"Tags","summary":"","title":"JNI"},{"content":"","date":null,"permalink":"/en/tags/ndk/","section":"Tags","summary":"","title":"NDK"},{"content":"","date":null,"permalink":"/en/categories/ndk/","section":"Categories","summary":"","title":"NDK"},{"content":"","date":null,"permalink":"/en/tags/android-operating-system/","section":"Tags","summary":"","title":"Android (operating system)"},{"content":"","date":null,"permalink":"/en/tags/cmake/","section":"Tags","summary":"","title":"CMake"},{"content":"Preface #CMake is a build tool, through which you can easily create cross-platform projects. It is usually used to build a project in two steps: generating a project file from the source code, and building the target product (which may be a dynamic library, a static library, or an executable program) from the project file. One of the main advantages of using CMake is that in the multi-platform or multi-people collaborative projects, developers can make their own preferences to make the choice of IDE, not subject to the influence of other people\u0026rsquo;s project configuration, it is a bit like a cross-platform IDE, through which the configuration of the relevant settings, you can be in multiple platforms seamlessly, improve development efficiency. \u0026lt;! \u0026mdash;more\u0026ndash;\u0026gt;\nSimplest CMake project #Project setup #A project managed with CMake will usually contain a CMakeLists.txt file in the project root directory, but of course subdirectories may also be present, which we\u0026rsquo;ll talk about later. Let\u0026rsquo;s start with the simplest project. Here is an example of the simplest project: CMakeProject | CMakeLists.txt | main.cpp This is the complete runnable minimal project. In order, let\u0026rsquo;s look at what\u0026rsquo;s in the file\nCMakeLists.txt ``\nSet the version number #cmake_minimum_required(VERSION 3.10)\nSet the project name #project(CMakeProject)\nSetting up product and source code associations #add_executable(${CMAKE_PROJECT_NAME} main.cpp) ``\nDescription:\n:: Commands in CMake are not case-sensitive :: Remarks beginning with `#' :: Referencing variable syntax ${variable name} So there are really only three lines of valid content in the document.\ncmake_minimum_required(VERSION 3.10) sets the minimum version supported by CMake. VERSION is the parameter name, followed by the version number. Note that the parameter name and parameters are separated by whitespace, not commas, or you will get an error. project(CMakeProject) CMake string can be with or without quotes, the effect is the same, this line is configured with the project name, such as the generation of Visual Studio project name is based on this name. add_executable(${CMAKE_PROJECT_NAME} main.cpp) is the real management of the source code and the target product of the place, here we use the reference variable writing, and the file does not define the variable, that this variable exists in CMake, in CMake there are a lot of predefined variables, we can be directly referenced in this way, the above writing is the name of the project is set to the product of the name, of course, you can also fill in the string directly, to take another name is fine. The latter main.cpp is used to generate the source path of the product, which is the most flexible part of CMake. This is the most flexible part of CMake. ** The source path can be various, it can be found out, written directly, relative path, absolute path, etc. ** If you have more than one source code, you can use it. ** If you have more than one source code, you can separate them with blanks and write them in order. In the configuration file above, we configured its source file as main.cpp, through which we want to generate an executable program with the same simple content:. #include \u0026lt;iostream\u0026gt; int main() { std::cout\u0026lt;\u0026lt;\u0026quot;hello CMake\u0026quot;\u0026lt;\u0026lt;std::endl; return 0; } Project compilation and execution #Now that the preparations are done, we\u0026rsquo;re going to use CMake to generate the executable.\nThe first step is of course to install CMake la, this is the download address !Download, according to their own platform to choose to download can be installed after the completion of the need to add it to the environment variables, so that we can easily use anywhere. After installing CMake, open the command line tool and go to the root directory of the project you just created, i.e. to the directory where CMakeLists.txt and main.cpp are stored, and prepare to generate the project in the next step.\nUsually in order not to affect and pollute the current working environment, we will choose to create a new directory to store the generated project files, the following I mainly Windows platform as the main platform to explain, other platforms are basically the same.\nmkdir build # Create a folder to store the project files; cd build #Switch the cmake working directory. cmake ... # Generate project files;\nAfter the execution of these three steps, we can see in the build folder has been generated inside a Visual Studio project, we can directly use Visual Studio to open the project, according to our habits to perform compilation and debugging. Of course, if you want to generate the fastest executable, I still recommend using CMake.\nTo perform a build with CMake, simply build on the previous step (i.e., you\u0026rsquo;ve already successfully performed the three steps above) by executing another command cmake --build . and you\u0026rsquo;re done. Remember to include the third period, which means that CMake builds in the current working directory. If everything went well in the above four steps, then we can see the executable file named after the first parameter of add_executable in the build/debug directory (in this case CMakeProject.exe), and we can execute it by double-clicking or dragging it to the command line.\nProject extensions #In the previous example, to generate the project file, we used two commands, in fact, here it can be done directly with one command - cmake build -S . -B build. The meaning of this command is to use the current path as the working path and the build directory as the build directory to generate the project file, that is, we don\u0026rsquo;t need to create the build folder manually. The -S parameter configures the source path and -B configures the build path.\nIn addition, since CMake does not have a cleanup method, every time we change CMake\u0026rsquo;s configuration (i.e., add or remove code from CMakeLists.txt) and need to regenerate the project file, we need to manually clean up the generated directory to make sure that it\u0026rsquo;s empty; if we don\u0026rsquo;t do this, the project may fail to be generated or the new configuration may not work. If you only change the source code, you don\u0026rsquo;t need to regenerate the project, just do the fourth step. Although the above operation is simple enough, but considering the long-term modification and validation needs, it is still too cumbersome and boring, especially to repeatedly switch the working directory, which is still rather annoying. So I recommend using batch processing to complete these operations. Combining the steps of cleaning up the generated directory and switching working directories, the final batch file may look like this ``\n@echo off rd /s /q build mkdir build cd build cmake \u0026hellip; cmake \u0026ndash;build . cd debug CMakeProject cd \u0026hellip; /\u0026hellip; `` Explain in order.\nThe first line turns off the command line\u0026rsquo;s echo, because we don\u0026rsquo;t want its echo to interfere with CMake\u0026rsquo;s message output in a way that causes unnecessary confusion, and also because usually we only care if it ends up doing its job rather than looking at what it\u0026rsquo;s doing.\nThe second line uses the Windows command for deleting folders (rmdir on Linux and MacOS), /s is to configure it to clear all the contents of the folder, including subfolders, without which the command will fail, and /q is to allow the command to execute the deletion directly without requiring us to manually confirm it, which is a very important parameter, as we would need to confirm the deletion of the folders one by one, completely defeating the purpose of automation. This parameter is very important, otherwise we need to confirm the deletion one by one, which completely defeats the purpose of automation. Then the next four sentences are what we talked about above, without further ado.\nComing all the way to the penultimate sentence, here I\u0026rsquo;ve written the name of the executable directly (you need to replace it with your own), in order to run the executable directly after the compilation is complete, which is useful for some applications that generate files.\nAt the end of the execution, then cut the directory back to the root directory of the project, this is the role of the last line, because we compile again has switched the directory to the generated directory, and the compiled executable file is in the generated directory of the subdirectory, so back to the root directory, we need to fall back twice, this is to ensure that the next time we can triumph in the execution of the batch processing key.\nSave the above as a file ending in bat, and then next time you can just type the bat file name at the command line to finish generating and building at once, it\u0026rsquo;s just awesome. This is all we need to know about CMake projects. Of course the actual project is far more complex than this, next I will use the pitfalls I have stepped on as a basis to increase the complexity of the project one by one, and slowly develop an understanding of CMake\u0026rsquo;s workflow.\nMulti-source projects #Personal insights #Before I start, let me talk about my understanding of the CMake project or CMakeLists.txt file. **We can\u0026rsquo;t understand a configuration in isolation, we need to categorize the commands and even distill its core working pattern. I am using the compilation and linking of c++ files as a clue to sort it out. ** We all know that for a c++ source file to generate executable code, it needs three steps\n:: Preprocessor processing, copying the contents of header files to source files, macro replacement, etc; The compiler compiles the source files into .o object files; The linker takes .o files and other libraries as input and links to generate executables. It is much easier to understand CMake along these lines. If CMake reports an error, we can use the information to find out which phase of CMake is causing the problem, and then quickly find a solution. In addition, we can also use this information to categorize the CMake configuration, my own understanding of the rough categorization is as follows.\n:: Configure CMake basic information: cmake_minimum_required; :: Source code management: file, aux_source_directory; :: Library-managed: find_libraray; :: Header file management: include_directories; :: For linkbase management: link_directories; :: Sub-project management: add_subdirectory; :: For generator management: add_executable, add_library; Of course, these are only a very small part of the picture, but they provide a better direction for understanding and searching for ideas to solve the problem.\nCMake manages subdirectories #Many times we introduce third-party packages to reduce duplicate coding efforts, and usually such code we need to put in other directories, so I created a new subdirectory to simulate the stored third-party code. For this case, we have two forms of inclusion - sub-modules and sub-directories.\nLet\u0026rsquo;s start with the simpler subdirectories. A subdirectory means that the third-party code is treated as part of our code and is merged and compiled together, in a way that makes our project look more compact. For example, the following project structure `` CMakeProject\n| auto.bat | CMakeLists.txt //Modify | main.cpp //modify |\n-\u0026ndash;3rd //Added lib.h ``\nI created a new subfolder to simulate the third party code, now let\u0026rsquo;s introduce it into main.cpp, compile it, and we\u0026rsquo;ll see that an error is reported, with the message fatal error C1083: Unable to open include file: \u0026quot;lib.h\u0026quot;: No such file or directory, which is normal. Combine this with the example I gave above. This error message is related to header files. Looking at the CMake documentation, I found out that CMake has an include_directories directive, which means to add the directory of the file header in order for CMake to find the header files. So I added include_directories(3rd) to the CMakeLists.txt file and ran the compile again and the project ran correctly. Take a look at the main.cpp at this point. `` #include #include \u0026lt;lib.h\u0026gt;\nint main() { int a=1,b=1; std::cout\u0026laquo;\u0026ldquo;hello CMake\u0026rdquo;\u0026laquo;std::endl; std::cout\u0026laquo;\u0026ldquo;a + b = \u0026ldquo;\u0026laquo;sum(a,b)\u0026laquo;std::endl; return 0; } `` Note: There is a one-to-one correspondence between include_directories and include in the cpp, i.e., if the directory configured in include_directories is . (the current directory, CMake does not add the current directory to the include path), then the include of the corresponding cpp should be written in the form of 3rd/lib.h, which simply means that include_directories is set to the root of the include directory. The other case is submodules.\nCMake management submodule #Submodule means that the module can be compiled separately and provided separately for other libraries to use, instead of being symbiotic with the main project, which applies to the case where the coupling with the main module is not too big. In order to satisfy this condition, we modify the directory structure to the following one `` CMakeProject\n| auto.bat | CMakeLists.txt //Modify | main.cpp\n| -3rd CMakeLists.txt //added lib.cpp //added lib.h //modify I changed the functions in `lib.h` to declarations, and the implementation is in the `lib.cpp` file. The biggest change was the creation of a new `CMakeLists.txt` file in the `3rd` directory, which manages all the source files in the `3rd` directory in a unified way (in case there are a lot of files, this is a simulation), and the use of `add_library` to package the `3rd` directory into sub-modules. project(sum) add_library(${PROJECT_NAME} lib.cpp) `` add_library can also specify the build type between the name and the source code, the default is STATIC, which is a static library, if you want to build a dynamic library you need to manually specify SHARED (add_library(${PROJECT_NAME} SHARED lib.cpp)).\nThe important changes come from CMakeLists.txt in the main directory. ``\nSet the version number #cmake_minimum_required(VERSION 3.10)\nSet the project name #project(CMakeProject)\nSpecify 3rd as the lookup directory for include #include_directories(3rd)\nSubmodules #add_subdirectory(3rd)\nSetting up product and source code associations #add_executable(${PROJECT_NAME} main.cpp) target_link_libraries(${PROJECT_NAME} sum) `` Added add_subdirectory, which is used to compile the source code from a specified directory as a module, provided there is a CMakeLists.txt file in that directory. Another change is the addition of target_link_libraries, which is used to link submodules into the main module; without it, linking would result in an error error LNK2019: unresolved external symbol. The name of the module needs to be the same as the first parameter of add_library in the submodule.\nCross-compilation #While the complexity of the project in the previous example was shown in the multiple directories and source code, the main complexity of the project in the process of cross-compiling with CMake is shown in the environment configuration. Although CMake can be used to cross-compile with little or no modification to CMakeLists.txt, newcomers are often overwhelmed by unfamiliar configurations and try to find an easy way to configure them with a single click. It is true that there is no such shortcut for CMake, but once we understand the essence of ** cross-compilation is the process of configuring property values correctly. However, once we understand that cross-compiling is the process of correctly configuring property values, the problem becomes clear. So, the questions above become familiar - what properties need to be configured, what are the appropriate values for those properties, how are those values passed to CMake, etc., and that\u0026rsquo;s what cross-compilation is all about. As mentioned before, CMake has a lot of preset variables, and we need to find some of these preset variables, set some values, and then let CMake do its job according to these configurations, and that\u0026rsquo;s what we need to do next. Below I will illustrate this process with an example of cross-compiling Android for Windows.\nPrepare yourself #On Windows platform, Visual Studio will be used as the compiler for C, C++ by default, which may report errors for compiling libraries for Android. So you need to use -G \u0026quot;Unix Makefiles\u0026quot; to change this behavior when executing cmake command. But that\u0026rsquo;s not enough, because CMake compilation is required to specify the compiler. The C,C++ compiler on Android is usually provided in the form of NDK, so we need to download the NDK, which provides us with two tools at the same time, one is the compiler, and the other one is android.toolchain.cmake, which is also the file composed of CMake commands, which specifies a lot of preset values for cross-compilation, which can greatly simplify the compilation process. This is also a CMake file, which specifies a lot of presets for cross-compilation, which can greatly reduce our work.\nWrite compilation scripts #As mentioned earlier, cross-compiling is all about changing the CMake preset, and there are two ways to change this preset that we\u0026rsquo;re going to use in combination. One is through the android.toolchain.cmake file provided by the NDK. The android.toolchain.cmake file sets most of the values, but it is also very flexible and has a lot of room for configuration. Therefore, depending on the user\u0026rsquo;s needs, we also need to pass some values dynamically when executing CMake commands in order for CMake to do its job correctly. This is another way - options. Passing options will start with -D followed by some CMake predefined variable Since there are a lot of options and most of them are complex, it\u0026rsquo;s better to document and modify them through script files. Below are just a few of the options that need to be specified to compile Android code on the Windows platform, and I\u0026rsquo;ll go over the necessary configurations one by one.\n-DCMAKE_SYSTEM_NAME=Android This configuration tells CMake that it needs to generate libraries for the Android platform, i.e., perform cross-compilation. The -DANDROID_ABI=x86 configuration tells CMake to generate libraries for the applicable architectural platforms. Readers familiar with Android development should not be unfamiliar with the supported values will change according to the changes in the NDK, such as the early armeabi has been removed in the NDK r17, now there are four mainstream armeabi-v7a, arm64-v8a, x86, x86_64. Just replace the values as needed. -DANDROID_PLATFORM=android-28, this value is not really necessary as there are preset values, but it is necessary to specify one for controllability. It is used to determine the minimum system version supported by the library. -DCMAKE_TOOLCHAIN_FILE=C:/Users/Leroene/AppData/Local/Android/Sdk/ndk/21.0.6113669/build/cmake/android.toolchain.cmake, which is the above mentioned preset file. Note that there are multiple files with this name in the NDK, and if you specify them incorrectly, you may get CMake errors, so my experience has been to change the version number (C:/Users/Leroene/AppData/Local/Android/Sdk/ndk/21.0.6113669) and the paths in front of it, and leave the paths behind it unchanged. The latter stays the same. -DCMAKE_MAKE_PROGRAM=C:/Users/Leroene/AppData/Local/Android/Sdk/ndk/21.0.6113669/prebuilt/windows-x86_64/bin/make The last argument specifies the path to the make program path, since we are specifying the code that generates the make project and Windows usually does not have a make executable, we need to let CMake find the make file in order to complete the compilation. My experience here is also to keep the later unchanged, modify the earlier, and keep the version consistent to avoid bugs. -DCMAKE_BUILD_TYPE=Release to specify the build type, which should be common enough. At this point all the configurations for cross compiling Android libraries for Windows have been explained. Let\u0026rsquo;s have a look at its complete example @echo off rd /s /q build mkdir build cd build cmake -G \u0026quot;Unix Makefiles\u0026quot; ^ -DCMAKE_TOOLCHAIN_FILE=C:/Users/Leroene/AppData/Local/Android/Sdk/ndk/21.0.6113669/build/cmake/android.toolchain.cmake ^ -DCMAKE_MAKE_PROGRAM=C:/Users/Leroene/AppData/Local/Android/Sdk/ndk/21.0.6113669/prebuilt/windows-x86_64/bin/make ^ -DANDROID_PLATFORM=android-28 ^ -DCMAKE_SYSTEM_NAME=Android ^ -DANDROID_ABI=x86 ^ -DCMAKE_BUILD_TYPE=Release ^ ... /3rd cmake --build . As you can see from the above, these options are all followed by a ^ symbol, which is not part of cmake, it is just written this way for our reading convenience, this is the command line break used for batch processing on Windows platform, its function is to tell the command parser that the command is not yet finished, and continue to be parsed down the line, this function is equivalent to \\ on Linux,MacOS This function corresponds to \\ on Linux and MacOS. Now that you have this configuration, how do you use it? It\u0026rsquo;s quite easy, just store these commands in the android.bat file, switch to the current directory in CMD, execute this file and you\u0026rsquo;ll find the static library file named libsum.a in the build directory. Next, let\u0026rsquo;s try to run in the emulator with this library file.\nUsing CMake in an Android project #In the Android platform, CMake is also used to manage jni projects, along with Gradle to complete the build. The biggest difference between this and a normal CMake project is that we usually need to reference multiple Android related libraries such as log, android, etc. These libraries are usually provided by NDK. These libraries are usually provided by the NDK, and we can just follow the default CMakeLists.txt file.\nDirectory structure #Next, for descriptive purposes, let\u0026rsquo;s take a look at the current directory structure (to avoid confusion, only the more representative files are listed here) CMakeProject │ android.bat │ CMakeLists.txt │ main.cpp │ ├óΓé¼╦£3rd │ CMakeLists.txt │ lib.cpp │ lib.h │ └─Android │ build.gradle │ ├─app │ │ build.gradle │ │ │ ├─libs │ └─src │ ├─main │ │ │ AndroidManifest.xml │ │ │ │ │ ├─cpp │ │ │ CMakeLists.txt │ │ │ native-lib.cpp │ │ │ │ │ ├─java │ │ └─me │ │ └─hongui │ │ └─cmakesum │ │ │ MainActivity.kt │ │ │ │ │ ├─jniLibs │ │ └─x86 │ │ │ libsum.a A new Android subdirectory has been created under the root of the original directory, which is an Android C++ project, so it has an extra cpp directory compared to other normal Android projects, and the main modifications we\u0026rsquo;ll make later on happen in that directory.\nThe original root directory, in order not to add complexity, exists only as a function of generating static libraries, so there are no modifications compared to the example above.\nBuild static libraries #First, let\u0026rsquo;s go back to the root directory. Use the android.bat batch in the root directory to generate static libraries that are available on Android, you can also modify the value of the -DANDROID_ABI option in the android.bat file to generate static libraries for other architectures, but this needs to correspond to the directories in the jniLibs directory, otherwise the link may fail. For example, the libsum.a file I generated is for the x86 architecture. Then you need to create a new x86 directory in the jniLibs directory, and then put libsum.a into that directory. This concludes the build of the static library.\nUsing static libraries #After putting the static libraries in place, we need to configure the build.gradle in the app directory and the CMakeLists.txt file in the cpp directory to complete the introduction of the static libraries.\nConfiguring Gradle #First, let\u0026rsquo;s talk about build.gradle, which is mainly concerned with modifying the ABI, because if you don\u0026rsquo;t specify it, the ABI generated by Gradle by default may not find the corresponding static library file to link to, which may lead to linking failure. The main changes in this file are as follows android { defaultConfig { externalNativeBuild { cmake { cppFlags \u0026quot;\u0026quot; abiFilters \u0026quot;x86\u0026quot; } } } } That is, specify the value of abiFilters as the same value as the static library you just built.\nConfiguring CMake #The CMakeLists.txt file is a bit more complicated, it needs to do two jobs, find the static libraries and static libraries\u0026rsquo; header files, and link the static libraries.\nfind header file #In the second part of the article we already knew about the include_directories command that lets CMake find header files, just set the parameter to the 3rd directory. It\u0026rsquo;s worth noting that CMake uses the current CMakeLists.txt file as its working directory, so to specify to the 3rd file we need to go all the way back in the directory to the root project, and end up with include_directories(... /... /... /... /... /3rd) configuration. Try to use relative paths, you can collaborate with multiple people without having to change the configuration.\nfind static library #The next step is to get CMake to find our static library. When it comes to libraries, it\u0026rsquo;s all related to add_library, the difference is just the parameters. When adding a library using source, we need to specify the name and source location of the library, whereas to reference a third-party library, we need to specify the name and type of the library, plus an IMPORTED indicator parameter to tell CMake that the library is imported. So there is a configuration like add_library(addSum STATIC IMPORTED).\nHowever, here we have only told CMake the name of the library, where the library is stored is not yet known, so we need another command to tell CMake where the library is stored. When it comes to configuration parameters, it\u0026rsquo;s usually the set_target_properties command, which can be called multiple times to set multiple configurations. set_target_properties(addSum PROPERTIES IMPORTED_LOCATION ${CMAKE_CURRENT_SOURCE_DIR}/... /jniLibs/${ANDROID_ABI}/libsum.a), the first parameter and the first parameter of the previous entry are one-to-one and can be taken at will. In fact, add_library is equivalent to generating a target product, and the first parameter is used to refer to such a product, which is why our set_target_properties is allowed to find the appropriate target setting properties. The second parameter is the standard way to write a configuration property, the third represents the property variable, and the fourth is the value of the property. The variable that configures the library path is IMPORTED_LOCATION, and the value has a pitfall here, as CMake under Android restricts the value to an absolute path, not a relative path. This is contrary to the original purpose of using CMake, fortunately, we have a few preset values that we can use, CMAKE_CURRENT_SOURCE_DIR is one of them, it represents the absolute path of the current CMakeLIsts.txt file, with this, and the directory fallback function, we can find any appropriate directory. At this point, a second problem arises, when there are multiple architectures with static libraries to configure, we introduce different directories, and there is a lot of duplication of configurations. Luckily, it helps to have ANDROID_ABI, which refers to a certain architecture that is currently being compiled, and as the compilation progresses, this value is set to the appropriate value, and is a one-to-one correspondence with the architecture being compiled. So, even though they are a bit strange, this gives me flexibility and simplicity.\nLinking static libraries #Now we have the header files and the libraries, but the compilation of C++ is divided into two steps, so far, our work has only done the compilation of things, not yet involved in the linking of things, of course, compared to the previous configuration, this is much simpler, undoubtedly is in the target_link_libraries command to add a parameter can be, such as target_link_libraries( native-lib ${log-lib} addSum ) Just yo note that the name corresponds one to one with the name configured during add_library.\nUse in source code #After a long wait, we are now able to introduce the addsum header file in the native-lib.cpp file and use the functions inside to get the job done. I intend for the function to return a string containing the result of the addition operation. The final implementation is as follows `` #include \u0026lt;jni.h\u0026gt; #include #include \u0026lt;lib.h\u0026gt;\nextern \u0026ldquo;C\u0026rdquo; JNIEXPORT jstring JNICALL Java_me_hongui_cmakesum_MainActivity_stringFromJNI( JNIEnv* env, jobject /* this */) { std::string hello = std::to_string(sum(1,1)); return env-\u0026gt;NewStringUTF(hello.c_str()); } `` At this point, click the run button on the toolbar and we can finally see the results of our static library work in the Android emulator.\nExtension #In fact, besides the way of referencing static libraries, we can also directly reference the source code by configuring the CMakeLists.txt file, which allows us to customize the source code anytime, anywhere, but it also slows down the compilation speed and may increase the complexity of CMakeLists.txt. So I still recommend the direct static library approach.\nSummarize #CMake in fact, there are many, many commands, we are involved here is only a very small part. However, I think that understanding CMake has these contents almost on it, the subsequent need to target learning on the line. Learning a technology, we must not be greedy, greedy for details. First of all, we must grasp the main, clear vein, the details of the latter is a matter of water to the drain. For CMake, I think it is the C++ code compiled into a binary process as the main trunk is enough. Where the source code comes from, where the header files are, where the library files are, how to organize the compilation, what libraries are involved in linking, what products are generated, and some general operations to complete these tasks, copying files ah, directory information ah, etc., a collection of these operations constitute the main body of CMake. In addition, CMake is really just a build tool, it is not a compiler or linker, and some problems may involve not only cmake, but also the compiler and linker. Of course, these are all issues that you may encounter later on, when you know more about it.\n","date":"9 August 2021","permalink":"/en/post/CMake%20Personal%20Understanding%20and%20Use.html","section":"Posts","summary":"Preface #CMake is a build tool, through which you can easily create cross-platform projects.","title":"CMake Personal Understanding and Use"},{"content":"","date":null,"permalink":"/en/tags/c/c++/","section":"Tags","summary":"","title":"C/C++"},{"content":"","date":null,"permalink":"/en/categories/c/c++/","section":"Categories","summary":"","title":"C/C++"}]